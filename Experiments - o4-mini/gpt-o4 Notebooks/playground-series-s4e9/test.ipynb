{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression of Used Car Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.75041, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 4s - 2ms/step - loss: 12.3391 - mae: 2.7249 - rmse: 3.5127 - val_loss: 0.7504 - val_mae: 0.7122 - val_rmse: 0.8663\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.75041 to 0.54282, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 5.8454 - mae: 1.9247 - rmse: 2.4177 - val_loss: 0.5428 - val_mae: 0.5688 - val_rmse: 0.7368\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.54282 to 0.45569, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 3.5368 - mae: 1.4943 - rmse: 1.8806 - val_loss: 0.4557 - val_mae: 0.5071 - val_rmse: 0.6750\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45569 to 0.37745, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 2.1124 - mae: 1.1452 - rmse: 1.4534 - val_loss: 0.3774 - val_mae: 0.4401 - val_rmse: 0.6144\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.37745 to 0.33741, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 1.1412 - mae: 0.8305 - rmse: 1.0683 - val_loss: 0.3374 - val_mae: 0.4109 - val_rmse: 0.5809\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.33741 to 0.31136, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 0.5927 - mae: 0.5837 - rmse: 0.7699 - val_loss: 0.3114 - val_mae: 0.3947 - val_rmse: 0.5580\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.31136 to 0.30957, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 0.3941 - mae: 0.4626 - rmse: 0.6278 - val_loss: 0.3096 - val_mae: 0.3974 - val_rmse: 0.5564\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30957 to 0.30434, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2357/2357 - 3s - 1ms/step - loss: 0.3574 - mae: 0.4359 - rmse: 0.5979 - val_loss: 0.3043 - val_mae: 0.3940 - val_rmse: 0.5517\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.30434\n",
      "2357/2357 - 4s - 2ms/step - loss: 0.3496 - mae: 0.4300 - rmse: 0.5913 - val_loss: 0.3079 - val_mae: 0.3992 - val_rmse: 0.5549\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3470 - mae: 0.4279 - rmse: 0.5891 - val_loss: 0.3114 - val_mae: 0.4010 - val_rmse: 0.5580\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.30434\n",
      "2357/2357 - 4s - 2ms/step - loss: 0.3462 - mae: 0.4273 - rmse: 0.5884 - val_loss: 0.3116 - val_mae: 0.4016 - val_rmse: 0.5582\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3461 - mae: 0.4275 - rmse: 0.5883 - val_loss: 0.3082 - val_mae: 0.3978 - val_rmse: 0.5551\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.30434\n",
      "2357/2357 - 4s - 2ms/step - loss: 0.3464 - mae: 0.4273 - rmse: 0.5886 - val_loss: 0.3096 - val_mae: 0.4005 - val_rmse: 0.5564\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3468 - mae: 0.4273 - rmse: 0.5889 - val_loss: 0.3047 - val_mae: 0.3961 - val_rmse: 0.5520\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3465 - mae: 0.4274 - rmse: 0.5886 - val_loss: 0.3063 - val_mae: 0.3977 - val_rmse: 0.5534\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.30434\n",
      "2357/2357 - 4s - 2ms/step - loss: 0.3470 - mae: 0.4275 - rmse: 0.5890 - val_loss: 0.3089 - val_mae: 0.4000 - val_rmse: 0.5558\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3450 - mae: 0.4265 - rmse: 0.5874 - val_loss: 0.3066 - val_mae: 0.3977 - val_rmse: 0.5537\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.30434\n",
      "2357/2357 - 3s - 1ms/step - loss: 0.3452 - mae: 0.4264 - rmse: 0.5875 - val_loss: 0.3089 - val_mae: 0.3989 - val_rmse: 0.5558\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m3928/3928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Data Loading\n",
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')\n",
    "sample_sub = pd.read_csv('sample_submission.csv.zip', nrows=1)\n",
    "\n",
    "# Infer ID and target columns\n",
    "cols = list(sample_sub.columns)\n",
    "id_col = cols[0]\n",
    "target_columns = cols[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = train_df.copy()\n",
    "\n",
    "# Target encoding for continuous regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "if np.all(y_values >= 0):\n",
    "    y_enc = np.log1p(y_values)\n",
    "else:\n",
    "    y_enc = y_values\n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc,\n",
    "    test_size=0.2,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Optional: drop ID from features\n",
    "for df_ in (X_train, X_val):\n",
    "    if id_col in df_.columns:\n",
    "        df_.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# Drop all-missing columns\n",
    "all_missing = [c for c in X_train.columns if X_train[c].isna().all()]\n",
    "X_train.drop(columns=all_missing, inplace=True)\n",
    "X_val.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# Feature types\n",
    "numeric_features = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "low_cardinality = [c for c in cat_features if X_train[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', cat_pipeline, low_cardinality)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture determination\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = y_train.shape[1] if y_train.ndim > 1 else 1\n",
    "\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    layer_sizes = [min(n_features*2, 128), min(n_features, 64)]\n",
    "    use_bn = False\n",
    "    drop_rates = [0.3, 0.3]\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    layer_sizes = [int(min(s, 1024)) for s in sizes if min(s, 1024) >= 16]\n",
    "    use_bn = True\n",
    "    drop_rates = [0.4] * len(layer_sizes)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "for idx, size in enumerate(layer_sizes):\n",
    "    if idx == 0:\n",
    "        model.add(Dense(size, activation='relu', input_dim=n_features))\n",
    "    else:\n",
    "        model.add(Dense(size, activation='relu'))\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop_rates[idx]))\n",
    "model.add(Dense(n_targets, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_data=(X_val_proc, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'training_RMSE': history.history['rmse'][-1],\n",
    "    'validation_RMSE': history.history['val_rmse'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "X_test = test_df.copy()\n",
    "if id_col in X_test.columns:\n",
    "    test_ids = X_test[id_col]\n",
    "X_test = X_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_test.drop(columns=all_missing, errors='ignore', inplace=True)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = np.expm1(np.clip(raw_preds, a_min=None, a_max=20)) if np.all(raw_preds >= 0) else raw_preds\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 06m 56s]\n",
      "val_loss: 0.2960398197174072\n",
      "\n",
      "Best val_loss So Far: 0.29548490047454834\n",
      "Total elapsed time: 01h 12m 03s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4714/4714 - 33s - 7ms/step - loss: 0.5952 - mae: 0.5276 - rmse: 0.7715 - val_loss: 0.3115 - val_mae: 0.4086 - val_rmse: 0.5581\n",
      "Epoch 2/100\n",
      "4714/4714 - 20s - 4ms/step - loss: 0.3683 - mae: 0.4491 - rmse: 0.6069 - val_loss: 0.3149 - val_mae: 0.4172 - val_rmse: 0.5612\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4714/4714 - 20s - 4ms/step - loss: 0.3568 - mae: 0.4400 - rmse: 0.5973 - val_loss: 0.2985 - val_mae: 0.3894 - val_rmse: 0.5464\n",
      "Epoch 4/100\n",
      "4714/4714 - 20s - 4ms/step - loss: 0.3379 - mae: 0.4253 - rmse: 0.5813 - val_loss: 0.3087 - val_mae: 0.3924 - val_rmse: 0.5556\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4714/4714 - 19s - 4ms/step - loss: 0.3303 - mae: 0.4196 - rmse: 0.5747 - val_loss: 0.2977 - val_mae: 0.3890 - val_rmse: 0.5457\n",
      "Epoch 6/100\n",
      "4714/4714 - 19s - 4ms/step - loss: 0.3279 - mae: 0.4171 - rmse: 0.5726 - val_loss: 0.3013 - val_mae: 0.3893 - val_rmse: 0.5489\n",
      "Epoch 7/100\n",
      "4714/4714 - 19s - 4ms/step - loss: 0.3265 - mae: 0.4162 - rmse: 0.5714 - val_loss: 0.3024 - val_mae: 0.3900 - val_rmse: 0.5499\n",
      "Epoch 8/100\n",
      "4714/4714 - 19s - 4ms/step - loss: 0.3258 - mae: 0.4158 - rmse: 0.5708 - val_loss: 0.3035 - val_mae: 0.3973 - val_rmse: 0.5509\n",
      "Epoch 9/100\n",
      "4714/4714 - 21s - 4ms/step - loss: 0.3239 - mae: 0.4142 - rmse: 0.5691 - val_loss: 0.3019 - val_mae: 0.3916 - val_rmse: 0.5495\n",
      "Epoch 10/100\n",
      "4714/4714 - 21s - 4ms/step - loss: 0.3244 - mae: 0.4146 - rmse: 0.5695 - val_loss: 0.3005 - val_mae: 0.3894 - val_rmse: 0.5482\n",
      "Epoch 11/100\n",
      "4714/4714 - 23s - 5ms/step - loss: 0.3227 - mae: 0.4136 - rmse: 0.5681 - val_loss: 0.3025 - val_mae: 0.3935 - val_rmse: 0.5500\n",
      "Epoch 12/100\n",
      "4714/4714 - 21s - 5ms/step - loss: 0.3234 - mae: 0.4125 - rmse: 0.5687 - val_loss: 0.3011 - val_mae: 0.3947 - val_rmse: 0.5487\n",
      "Epoch 13/100\n",
      "4714/4714 - 22s - 5ms/step - loss: 0.3214 - mae: 0.4120 - rmse: 0.5669 - val_loss: 0.3007 - val_mae: 0.3899 - val_rmse: 0.5484\n",
      "Epoch 14/100\n",
      "4714/4714 - 25s - 5ms/step - loss: 0.3209 - mae: 0.4114 - rmse: 0.5665 - val_loss: 0.3015 - val_mae: 0.3956 - val_rmse: 0.5491\n",
      "Epoch 15/100\n",
      "4714/4714 - 23s - 5ms/step - loss: 0.3205 - mae: 0.4111 - rmse: 0.5661 - val_loss: 0.2997 - val_mae: 0.3920 - val_rmse: 0.5475\n",
      "\u001b[1m3899/3928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 00:26:13.733668: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:13.745093: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 6260 bytes spill stores, 6320 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:13.816713: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 216 bytes spill stores, 216 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:13.826874: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26_0', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:13.985713: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 376 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.063573: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.124714: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 428 bytes spill stores, 428 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.282093: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 952 bytes spill stores, 952 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.327811: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 1028 bytes spill stores, 1028 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.428591: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 4744 bytes spill stores, 4800 bytes spill loads\n",
      "\n",
      "2025-07-16 00:26:14.458974: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 1020 bytes spill stores, 1020 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3928/3928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Data Loading\n",
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')\n",
    "sample_sub = pd.read_csv('sample_submission.csv.zip', nrows=1)\n",
    "\n",
    "# Infer ID and target columns\n",
    "cols = list(sample_sub.columns)\n",
    "id_col = cols[0]\n",
    "target_columns = cols[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = train_df.copy()\n",
    "\n",
    "# Target encoding for continuous regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "if np.all(y_values >= 0):\n",
    "    y_enc = np.log1p(y_values)\n",
    "else:\n",
    "    y_enc = y_values\n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc,\n",
    "    test_size=0.2,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Optional: drop ID from features\n",
    "for df_ in (X_train, X_val):\n",
    "    if id_col in df_.columns:\n",
    "        df_.drop(columns=[id_col], inplace=True)\n",
    "\n",
    "# Drop all-missing columns\n",
    "all_missing = [c for c in X_train.columns if X_train[c].isna().all()]\n",
    "X_train.drop(columns=all_missing, inplace=True)\n",
    "X_val.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# Feature types\n",
    "numeric_features = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "low_cardinality = [c for c in cat_features if X_train[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', cat_pipeline, low_cardinality)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture determination\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = y_train.shape[1] if y_train.ndim > 1 else 1\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# Define early stopping and checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "# HyperModel\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        model = Sequential()\n",
    "        for idx in range(layers):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(units, activation='relu', input_dim=n_features))\n",
    "            else:\n",
    "                model.add(Dense(units, activation='relu'))\n",
    "            model.add(Dropout(drop))\n",
    "        model.add(Dense(1, activation='linear'))  # Assuming n_targets = 1 for regression\n",
    "\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "start_time = time.time()\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'training_RMSE': history.history['rmse'][-1],\n",
    "    'validation_RMSE': history.history['val_rmse'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "X_test = test_df.copy()\n",
    "if id_col in X_test.columns:\n",
    "    test_ids = X_test[id_col]\n",
    "X_test = X_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_test.drop(columns=all_missing, errors='ignore', inplace=True)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = np.expm1(np.clip(raw_preds, a_min=None, a_max=20)) if np.all(raw_preds >= 0) else raw_preds\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327.234264\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
