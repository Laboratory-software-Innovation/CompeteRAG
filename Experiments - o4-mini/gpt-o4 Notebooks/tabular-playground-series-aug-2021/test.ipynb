{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Playground Series - Aug 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 22:23:16.805563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752099796.829364  922662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752099796.836460  922662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752099796.857670  922662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752099796.857697  922662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752099796.857699  922662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752099796.857702  922662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 22:23:16.865304: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 22:23:33.533514: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.23649, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 14s - 4ms/step - loss: 1.5106 - mae: 1.0308 - rmse: 1.2291 - val_loss: 1.2365 - val_mae: 0.9643 - val_rmse: 1.1120\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 1.23649 to 1.21839, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 10s - 3ms/step - loss: 1.2209 - mae: 0.9552 - rmse: 1.1049 - val_loss: 1.2184 - val_mae: 0.9575 - val_rmse: 1.1038\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 1.21839 to 1.21103, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 11s - 3ms/step - loss: 1.2050 - mae: 0.9488 - rmse: 1.0977 - val_loss: 1.2110 - val_mae: 0.9545 - val_rmse: 1.1005\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 1.21103 to 1.20415, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 10s - 3ms/step - loss: 1.1975 - mae: 0.9456 - rmse: 1.0943 - val_loss: 1.2041 - val_mae: 0.9514 - val_rmse: 1.0973\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 1.20415 to 1.19617, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 10s - 3ms/step - loss: 1.1927 - mae: 0.9434 - rmse: 1.0921 - val_loss: 1.1962 - val_mae: 0.9472 - val_rmse: 1.0937\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 1.19617 to 1.19380, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 10s - 3ms/step - loss: 1.1864 - mae: 0.9405 - rmse: 1.0892 - val_loss: 1.1938 - val_mae: 0.9458 - val_rmse: 1.0926\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 1.19380 to 1.19228, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 10s - 3ms/step - loss: 1.1827 - mae: 0.9387 - rmse: 1.0875 - val_loss: 1.1923 - val_mae: 0.9447 - val_rmse: 1.0919\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 1.19228 to 1.19173, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 11s - 3ms/step - loss: 1.1781 - mae: 0.9363 - rmse: 1.0854 - val_loss: 1.1917 - val_mae: 0.9431 - val_rmse: 1.0917\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.19173\n",
      "3125/3125 - 10s - 3ms/step - loss: 1.1751 - mae: 0.9349 - rmse: 1.0840 - val_loss: 1.1926 - val_mae: 0.9432 - val_rmse: 1.0921\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.19173\n",
      "3125/3125 - 10s - 3ms/step - loss: 1.1727 - mae: 0.9337 - rmse: 1.0829 - val_loss: 1.1921 - val_mae: 0.9434 - val_rmse: 1.0919\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.19173\n",
      "3125/3125 - 10s - 3ms/step - loss: 1.1696 - mae: 0.9325 - rmse: 1.0815 - val_loss: 1.1956 - val_mae: 0.9442 - val_rmse: 1.0935\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 3ms/step - loss: 1.1688 - mae: 0.9321 - rmse: 1.0811 - val_loss: 1.1937 - val_mae: 0.9433 - val_rmse: 1.0926\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 3ms/step - loss: 1.1672 - mae: 0.9310 - rmse: 1.0804 - val_loss: 1.1925 - val_mae: 0.9430 - val_rmse: 1.0920\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 3ms/step - loss: 1.1643 - mae: 0.9296 - rmse: 1.0790 - val_loss: 1.1943 - val_mae: 0.9432 - val_rmse: 1.0928\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 4ms/step - loss: 1.1619 - mae: 0.9285 - rmse: 1.0779 - val_loss: 1.1955 - val_mae: 0.9436 - val_rmse: 1.0934\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 3ms/step - loss: 1.1615 - mae: 0.9283 - rmse: 1.0777 - val_loss: 1.1932 - val_mae: 0.9436 - val_rmse: 1.0923\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.19173\n",
      "3125/3125 - 11s - 3ms/step - loss: 1.1591 - mae: 0.9268 - rmse: 1.0766 - val_loss: 1.1971 - val_mae: 0.9441 - val_rmse: 1.0941\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.19173\n",
      "3125/3125 - 10s - 3ms/step - loss: 1.1573 - mae: 0.9260 - rmse: 1.0758 - val_loss: 1.1962 - val_mae: 0.9440 - val_rmse: 1.0937\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 871us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Reproducibility\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, SparseTopKCategoricalAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 2. Data Loading\n",
    "train_df = pd.read_csv('tabular-playground-series-aug-2021/train.csv.zip')\n",
    "df_test = pd.read_csv('tabular-playground-series-aug-2021/test.csv.zip')\n",
    "id_col = 'id'\n",
    "target_columns = ['loss']\n",
    "\n",
    "# 3. Target Encoding for regression\n",
    "y_values = train_df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 4. Features\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = df_test[id_col]\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# 4a. Drop columns with all missing values\n",
    "all_missing = X.columns[X.isna().all()].tolist()\n",
    "X.drop(columns=all_missing, inplace=True)\n",
    "X_test.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# 4b. Identify and drop categorical columns with high cardinality\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if X[col].nunique() > 50:\n",
    "        X.drop(columns=col, inplace=True)\n",
    "        X_test.drop(columns=col, inplace=True)\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# 6. Model Architecture for continuous regression\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    hidden_sizes = [min(n_features * 2, 128), min(n_features, 64)]\n",
    "    dropout_rate = 0.3\n",
    "    use_bn = False\n",
    "else:\n",
    "    sizes = [int(min(n_features * i, 1024)) for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_sizes = [s for s in sizes if s >= 16]\n",
    "    dropout_rate = 0.4\n",
    "    use_bn = True\n",
    "\n",
    "inputs = tf.keras.Input(shape=(n_features,))\n",
    "x = inputs\n",
    "for hs in hidden_sizes:\n",
    "    x = tf.keras.layers.Dense(hs, activation='relu')(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# 7. Callbacks & Training\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# 8. Evaluation & Logging\n",
    "hist = history.history\n",
    "results = {\n",
    "    'training_accuracy': hist['rmse'][-1],\n",
    "    'training_loss': hist['loss'][-1],\n",
    "    'validation_accuracy': hist['val_rmse'][-1],\n",
    "    'validation_loss': hist['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 9. Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(np.clip(final, a_min=None, a_max=20))\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 07m 49s]\n",
      "val_loss: 1.2046785354614258\n",
      "\n",
      "Best val_loss So Far: 1.199518084526062\n",
      "Total elapsed time: 01h 23m 53s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 49s - 8ms/step - loss: 1.3338 - mae: 0.9855 - rmse: 1.1549 - val_loss: 1.2267 - val_mae: 0.9566 - val_rmse: 1.1075\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 37s - 6ms/step - loss: 1.2200 - mae: 0.9529 - rmse: 1.1045 - val_loss: 1.2201 - val_mae: 0.9547 - val_rmse: 1.1046\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 34s - 5ms/step - loss: 1.2069 - mae: 0.9484 - rmse: 1.0986 - val_loss: 1.2087 - val_mae: 0.9502 - val_rmse: 1.0994\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 38s - 6ms/step - loss: 1.1962 - mae: 0.9442 - rmse: 1.0937 - val_loss: 1.2026 - val_mae: 0.9484 - val_rmse: 1.0966\n",
      "Epoch 5/100\n",
      "6250/6250 - 40s - 6ms/step - loss: 1.1867 - mae: 0.9402 - rmse: 1.0894 - val_loss: 1.2038 - val_mae: 0.9484 - val_rmse: 1.0972\n",
      "Epoch 6/100\n",
      "6250/6250 - 36s - 6ms/step - loss: 1.1781 - mae: 0.9360 - rmse: 1.0854 - val_loss: 1.2078 - val_mae: 0.9507 - val_rmse: 1.0990\n",
      "Epoch 7/100\n",
      "6250/6250 - 34s - 5ms/step - loss: 1.1705 - mae: 0.9324 - rmse: 1.0819 - val_loss: 1.2066 - val_mae: 0.9497 - val_rmse: 1.0985\n",
      "Epoch 8/100\n",
      "6250/6250 - 35s - 6ms/step - loss: 1.1638 - mae: 0.9292 - rmse: 1.0788 - val_loss: 1.2095 - val_mae: 0.9497 - val_rmse: 1.0998\n",
      "Epoch 9/100\n",
      "6250/6250 - 37s - 6ms/step - loss: 1.1587 - mae: 0.9265 - rmse: 1.0764 - val_loss: 1.2089 - val_mae: 0.9492 - val_rmse: 1.0995\n",
      "Epoch 10/100\n",
      "6250/6250 - 38s - 6ms/step - loss: 1.1524 - mae: 0.9232 - rmse: 1.0735 - val_loss: 1.2111 - val_mae: 0.9496 - val_rmse: 1.1005\n",
      "Epoch 11/100\n",
      "6250/6250 - 34s - 5ms/step - loss: 1.1475 - mae: 0.9207 - rmse: 1.0712 - val_loss: 1.2144 - val_mae: 0.9508 - val_rmse: 1.1020\n",
      "Epoch 12/100\n",
      "6250/6250 - 35s - 6ms/step - loss: 1.1417 - mae: 0.9177 - rmse: 1.0685 - val_loss: 1.2162 - val_mae: 0.9508 - val_rmse: 1.1028\n",
      "Epoch 13/100\n",
      "6250/6250 - 38s - 6ms/step - loss: 1.1378 - mae: 0.9159 - rmse: 1.0667 - val_loss: 1.2159 - val_mae: 0.9507 - val_rmse: 1.1027\n",
      "Epoch 14/100\n",
      "6250/6250 - 39s - 6ms/step - loss: 1.1330 - mae: 0.9134 - rmse: 1.0644 - val_loss: 1.2286 - val_mae: 0.9537 - val_rmse: 1.1084\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# 1. Reproducibility\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError, SparseTopKCategoricalAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 2. Data Loading\n",
    "train_df = pd.read_csv('train.csv.zip')\n",
    "df_test = pd.read_csv('test.csv.zip')\n",
    "id_col = 'id'\n",
    "target_columns = ['loss']\n",
    "\n",
    "# 3. Target Encoding for regression\n",
    "y_values = train_df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 4. Features\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = df_test[id_col]\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# 4a. Drop columns with all missing values\n",
    "all_missing = X.columns[X.isna().all()].tolist()\n",
    "X.drop(columns=all_missing, inplace=True)\n",
    "X_test.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# 4b. Identify and drop categorical columns with high cardinality\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if X[col].nunique() > 50:\n",
    "        X.drop(columns=col, inplace=True)\n",
    "        X_test.drop(columns=col, inplace=True)\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# 6. Model Architecture for continuous regression\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    hidden_sizes = [min(n_features * 2, 128), min(n_features, 64)]\n",
    "    dropout_rate = 0.3\n",
    "    use_bn = False\n",
    "else:\n",
    "    sizes = [int(min(n_features * i, 1024)) for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_sizes = [s for s in sizes if s >= 16]\n",
    "    dropout_rate = 0.4\n",
    "    use_bn = True\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "# HyperModel\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, 0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # Example batch size, adjust as needed\n",
    "ep = 100  # Example epochs, adjust as needed\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if 'y_val' in locals() and y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_test_proc, y_test),  # Assuming y_test is defined\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "if 'y_val' in locals() and y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_test_proc, y_test),  # Assuming y_test is defined\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "duration = time.time() - start_time  # Calculate duration\n",
    "\n",
    "# 8. Evaluation & Logging\n",
    "hist = history.history\n",
    "results = {\n",
    "    'training_accuracy': hist['rmse'][-1],\n",
    "    'training_loss': hist['loss'][-1],\n",
    "    'validation_accuracy': hist['val_rmse'][-1],\n",
    "    'validation_loss': hist['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 9. Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(np.clip(final, a_min=None, a_max=20))\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525.935126\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
