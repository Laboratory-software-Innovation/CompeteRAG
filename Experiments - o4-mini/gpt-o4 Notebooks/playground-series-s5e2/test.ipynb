{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpack Prediction Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 21:25:04.887946: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.33582, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49929/49929 - 80s - 2ms/step - loss: 0.4076 - mae: 0.5157 - rmse: 0.6384 - val_loss: 0.3358 - val_mae: 0.4837 - val_rmse: 0.5795\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.33582 to 0.33579, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49929/49929 - 85s - 2ms/step - loss: 0.3356 - mae: 0.4828 - rmse: 0.5793 - val_loss: 0.3358 - val_mae: 0.4835 - val_rmse: 0.5795\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.33579 to 0.33577, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49929/49929 - 87s - 2ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5793 - val_loss: 0.3358 - val_mae: 0.4835 - val_rmse: 0.5795\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.33577\n",
      "49929/49929 - 84s - 2ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5792 - val_loss: 0.3358 - val_mae: 0.4839 - val_rmse: 0.5795\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.33577\n",
      "49929/49929 - 82s - 2ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5792 - val_loss: 0.3358 - val_mae: 0.4837 - val_rmse: 0.5795\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.33577\n",
      "49929/49929 - 74s - 1ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5792 - val_loss: 0.3358 - val_mae: 0.4837 - val_rmse: 0.5795\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.33577\n",
      "49929/49929 - 71s - 1ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5792 - val_loss: 0.3358 - val_mae: 0.4838 - val_rmse: 0.5795\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.33577\n",
      "49929/49929 - 70s - 1ms/step - loss: 0.3355 - mae: 0.4827 - rmse: 0.5792 - val_loss: 0.3358 - val_mae: 0.4837 - val_rmse: 0.5795\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 578us/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1. Data Loading & Split\n",
    "# Training files\n",
    "train_files = ['playground-series-s5e2/train.csv', 'playground-series-s5e2/training_extra.csv']\n",
    "train_dfs = [pd.read_csv(f) for f in train_files]\n",
    "# Test file\n",
    "df_test = pd.read_csv('playground-series-s5e2/test.csv')\n",
    "# Infer id and target columns from sample submission\n",
    "sub_sample = pd.read_csv('playground-series-s5e2/sample_submission.csv', nrows=0)\n",
    "id_col = sub_sample.columns[0]\n",
    "target_columns = sub_sample.columns.tolist()[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Encoding (Regression)\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 3. Features & IDs\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Drop columns with all missing values\n",
    "X_train.dropna(axis=1, how='all', inplace=True)\n",
    "X_val = X_val[X_train.columns]\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cats = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, low_card_cats)\n",
    "])\n",
    "# Fit & transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# 6. Model Architecture Selection\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = len(target_columns)\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units1 = min(n_features * 2, 128)\n",
    "    units2 = min(n_features, 64)\n",
    "    hidden_layers = [int(units1), int(units2)]\n",
    "    use_bn = False\n",
    "    dropout_rate = 0.3\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_layers = [int(s) for s in sizes if s >= 16]\n",
    "    use_bn = True\n",
    "    dropout_rate = 0.4\n",
    "\n",
    "# Build the model\n",
    "inputs = Input(shape=(n_features,))\n",
    "x = inputs\n",
    "for units in hidden_layers:\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    if use_bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(n_targets, activation='linear')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# 7. Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# 8. Callbacks & Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# 9. Evaluation & Logging\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_rmse = history.history['rmse'][-1]\n",
    "validation_loss = history.history['val_loss'][-1]\n",
    "validation_rmse = history.history['val_rmse'][-1]\n",
    "results = {\n",
    "    'training_loss': training_loss,\n",
    "    'training_rmse': training_rmse,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_rmse': validation_rmse,\n",
    "    'training_duration': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 10. Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(final)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner - 2 Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1 - Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 03m 10s]\n",
      "val_loss: 0.3359396457672119\n",
      "\n",
      "Best val_loss So Far: 0.3359396457672119\n",
      "Total elapsed time: 00h 03m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99858/99858 - 186s - 2ms/step - loss: 0.3556 - mae: 0.4931 - val_loss: 0.3360 - val_mae: 0.4836\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rmse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# 9. Evaluation & Logging\u001b[39;00m\n\u001b[32m    170\u001b[39m training_loss = history.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m training_rmse = \u001b[43mhistory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrmse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[-\u001b[32m1\u001b[39m]\n\u001b[32m    172\u001b[39m validation_loss = history.history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m    173\u001b[39m validation_rmse = history.history[\u001b[33m'\u001b[39m\u001b[33mval_rmse\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'rmse'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1. Data Loading & Split\n",
    "# Training files\n",
    "train_files = ['playground-series-s5e2/train.csv', 'playground-series-s5e2/training_extra.csv']\n",
    "train_dfs = [pd.read_csv(f) for f in train_files]\n",
    "# Test file\n",
    "df_test = pd.read_csv('playground-series-s5e2/test.csv')\n",
    "# Infer id and target columns from sample submission\n",
    "sub_sample = pd.read_csv('playground-series-s5e2/sample_submission.csv', nrows=0)\n",
    "id_col = sub_sample.columns[0]\n",
    "target_columns = sub_sample.columns.tolist()[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Encoding (Regression)\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 3. Features & IDs\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Drop columns with all missing values\n",
    "X_train.dropna(axis=1, how='all', inplace=True)\n",
    "X_val = X_val[X_train.columns]\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cats = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, low_card_cats)\n",
    "])\n",
    "# Fit & transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# 6. Model Architecture Selection\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = len(target_columns)\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units1 = min(n_features * 2, 128)\n",
    "    units2 = min(n_features, 64)\n",
    "    hidden_layers = [int(units1), int(units2)]\n",
    "    use_bn = False\n",
    "    dropout_rate = 0.3\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_layers = [int(s) for s in sizes if s >= 16]\n",
    "    use_bn = True\n",
    "    dropout_rate = 0.4\n",
    "\n",
    "# Build the model using Keras Tuner\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, 0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "# Initialize the Bayesian tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20   # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain the model with the original callbacks and data\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# 9. Evaluation & Logging\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_rmse = history.history['rmse'][-1]\n",
    "validation_loss = history.history['val_loss'][-1]\n",
    "validation_rmse = history.history['val_rmse'][-1]\n",
    "results = {\n",
    "    'training_loss': training_loss,\n",
    "    'training_rmse': training_rmse,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_rmse': validation_rmse,\n",
    "    'training_duration': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 10. Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(final)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 07m 58s]\n",
      "\n",
      "Best val_loss So Far: 0.335781455039978\n",
      "Total elapsed time: 11h 08m 19s\n",
      "\n",
      "Search: Running Trial #9\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |6                 |layers\n",
      "512               |64                |units\n",
      "0.3               |0.2               |dropout\n",
      "adam              |adam              |optimizer\n",
      "0.0023691         |0.0046217         |learning_rate\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m99858/99858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3771 - mae: 0.5044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99858/99858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 4ms/step - loss: 0.3771 - mae: 0.5044 - val_loss: 0.3360 - val_mae: 0.4840\n",
      "Epoch 2/20\n",
      "\u001b[1m99854/99858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3368 - mae: 0.4837"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99858/99858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 4ms/step - loss: 0.3368 - mae: 0.4837 - val_loss: 0.3360 - val_mae: 0.4837\n",
      "Epoch 3/20\n",
      "\u001b[1m99858/99858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 4ms/step - loss: 0.3357 - mae: 0.4829 - val_loss: 0.3360 - val_mae: 0.4837\n",
      "Epoch 4/20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1. Data Loading & Split\n",
    "# Training files\n",
    "train_files = ['train.csv', 'training_extra.csv']\n",
    "train_dfs = [pd.read_csv(f) for f in train_files]\n",
    "# Test file\n",
    "df_test = pd.read_csv('test.csv')\n",
    "# Infer id and target columns from sample submission\n",
    "sub_sample = pd.read_csv('sample_submission.csv', nrows=0)\n",
    "id_col = sub_sample.columns[0]\n",
    "target_columns = sub_sample.columns.tolist()[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Encoding (Regression)\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 3. Features & IDs\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Drop columns with all missing values\n",
    "X_train.dropna(axis=1, how='all', inplace=True)\n",
    "X_val = X_val[X_train.columns]\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cats = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, low_card_cats)\n",
    "])\n",
    "# Fit & transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# 6. Model Architecture Selection\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = len(target_columns)\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units1 = min(n_features * 2, 128)\n",
    "    units2 = min(n_features, 64)\n",
    "    hidden_layers = [int(units1), int(units2)]\n",
    "    use_bn = False\n",
    "    dropout_rate = 0.3\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_layers = [int(s) for s in sizes if s >= 16]\n",
    "    use_bn = True\n",
    "    dropout_rate = 0.4\n",
    "\n",
    "# Build the model using Keras Tuner\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, 0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "# Initialize the Bayesian tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20   # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain the model with the original callbacks and data\n",
    "start_time = time.time()  # Start timing\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "end_time = time.time()  # End timing\n",
    "duration = end_time - start_time  # Calculate duration\n",
    "\n",
    "# 9. Evaluation & Logging\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_mae = history.history['mae'][-1]  # Changed from 'rmse' to 'mae'\n",
    "validation_loss = history.history['val_loss'][-1]\n",
    "validation_mae = history.history['val_mae'][-1]  # Changed from 'val_rmse' to 'val_mae'\n",
    "results = {\n",
    "    'training_loss': training_loss,\n",
    "    'training_mae': training_mae,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_mae': validation_mae,\n",
    "    'training_duration': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 10. Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(final)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
