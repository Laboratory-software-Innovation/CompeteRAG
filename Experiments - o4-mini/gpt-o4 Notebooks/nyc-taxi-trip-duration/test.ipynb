{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Taxi Trip Duration - Attempts 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dropoff_datetime'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 1. Drop columns with all missing values\u001b[39;00m\n\u001b[32m     45\u001b[39m X = X.loc[:, X.isnull().mean() < \u001b[32m1.0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m X_test = \u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 2. Extract time features\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df_ \u001b[38;5;129;01min\u001b[39;00m [X, X_test]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['dropoff_datetime'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Data Loading\n",
    "# Assuming CSVs are extracted from the provided ZIPs\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Identify columns\n",
    "id_col = 'id'\n",
    "target_columns = ['trip_duration']\n",
    "\n",
    "# Prepare training data\n",
    "df = df_train.copy()\n",
    "# Log-transform target for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values)\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Prepare test features\n",
    "test_ids = df_test[id_col]\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Drop columns with all missing values\n",
    "X = X.loc[:, X.isnull().mean() < 1.0]\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "# 2. Extract time features\n",
    "for df_ in [X, X_test]:\n",
    "    df_['pickup_datetime'] = pd.to_datetime(df_['pickup_datetime'])\n",
    "    df_['hour'] = df_['pickup_datetime'].dt.hour\n",
    "    df_['dayofweek'] = df_['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# 3. Compute haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "for df_ in [X, X_test]:\n",
    "    df_['haversine'] = haversine_distance(\n",
    "        df_['pickup_latitude'], df_['pickup_longitude'],\n",
    "        df_['dropoff_latitude'], df_['dropoff_longitude']\n",
    "    )\n",
    "\n",
    "# 4. Drop raw datetime & coordinate columns\n",
    "drop_cols = ['pickup_datetime', 'dropoff_datetime',\n",
    "             'pickup_latitude', 'pickup_longitude',\n",
    "             'dropoff_latitude', 'dropoff_longitude']\n",
    "X = X.drop(columns=[c for c in drop_cols if c in X])\n",
    "X_test = X_test.drop(columns=[c for c in drop_cols if c in X_test])\n",
    "\n",
    "# Train/Validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Drop high-cardinality cats\n",
    "cat_cols = [c for c in cat_cols if X_train[c].nunique() < 50]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model Architecture for small feature set\n",
    "n_features = X_train_proc.shape[1]\n",
    "layer1 = min(n_features * 2, 128)\n",
    "layer2 = min(n_features, 64)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(n_features,)),\n",
    "    tf.keras.layers.Dense(layer1, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(layer2, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='root_mean_squared_error'), MeanAbsoluteError(name='mean_absolute_error')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Training\n",
    "t0 = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_data=(X_val_proc, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "train_duration = time.time() - t0\n",
    "\n",
    "# Evaluation & Logging\n",
    "results = {\n",
    "    'training_accuracy': history.history['root_mean_squared_error'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_root_mean_squared_error'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final_preds = np.expm1(np.clip(raw_preds, a_min=None, a_max=20))\n",
    "if final_preds.ndim == 1:\n",
    "    final_preds = final_preds.reshape(-1, 1)\n",
    "\n",
    "submission = pd.DataFrame(final_preds, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2 - Follow-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dropoff_datetime'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 1. Drop columns with all missing values\u001b[39;00m\n\u001b[32m     45\u001b[39m X = X.loc[:, X.isnull().mean() < \u001b[32m1.0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m X_test = \u001b[43mX_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Fixed this line\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 2. Extract time features\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df_ \u001b[38;5;129;01min\u001b[39;00m [X, X_test]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1377\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take_opportunity(tup):\n\u001b[32m   1375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1020\u001b[39m, in \u001b[36m_LocationIndexer._getitem_tuple_same_dim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_null_slice(key):\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m retval = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m retval.ndim == \u001b[38;5;28mself\u001b[39m.ndim\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1555\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['dropoff_datetime'] not in index\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Data Loading\n",
    "# Assuming CSVs are extracted from the provided ZIPs\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Identify columns\n",
    "id_col = 'id'\n",
    "target_columns = ['trip_duration']\n",
    "\n",
    "# Prepare training data\n",
    "df = df_train.copy()\n",
    "# Log-transform target for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values)\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Prepare test features\n",
    "test_ids = df_test[id_col]\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Drop columns with all missing values\n",
    "X = X.loc[:, X.isnull().mean() < 1.0]\n",
    "X_test = X_test.loc[:, X.columns]  # Fixed this line\n",
    "\n",
    "# 2. Extract time features\n",
    "for df_ in [X, X_test]:\n",
    "    df_['pickup_datetime'] = pd.to_datetime(df_['pickup_datetime'])\n",
    "    df_['hour'] = df_['pickup_datetime'].dt.hour\n",
    "    df_['dayofweek'] = df_['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# 3. Compute haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "for df_ in [X, X_test]:\n",
    "    df_['haversine'] = haversine_distance(\n",
    "        df_['pickup_latitude'], df_['pickup_longitude'],\n",
    "        df_['dropoff_latitude'], df_['dropoff_longitude']\n",
    "    )\n",
    "\n",
    "# 4. Drop raw datetime & coordinate columns\n",
    "drop_cols = ['pickup_datetime', 'dropoff_datetime',\n",
    "             'pickup_latitude', 'pickup_longitude',\n",
    "             'dropoff_latitude', 'dropoff_longitude']\n",
    "X = X.drop(columns=[c for c in drop_cols if c in X])\n",
    "X_test = X_test.drop(columns=[c for c in drop_cols if c in X_test])\n",
    "\n",
    "# Train/Validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Drop high-cardinality cats\n",
    "cat_cols = [c for c in cat_cols if X_train[c].nunique() < 50]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model Architecture for small feature set\n",
    "n_features = X_train_proc.shape[1]\n",
    "layer1 = min(n_features * 2, 128)\n",
    "layer2 = min(n_features, 64)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(n_features,)),\n",
    "    tf.keras.layers.Dense(layer1, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(layer2, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='root_mean_squared_error'), MeanAbsoluteError(name='mean_absolute_error')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "# Training\n",
    "t0 = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_data=(X_val_proc, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "train_duration = time.time() - t0\n",
    "\n",
    "# Evaluation & Logging\n",
    "results = {\n",
    "    'training_accuracy': history.history['root_mean_squared_error'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_root_mean_squared_error'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final_preds = np.expm1(np.clip(raw_preds, a_min=None, a_max=20))\n",
    "if final_preds.ndim == 1:\n",
    "    final_preds = final_preds.reshape(-1, 1)\n",
    "\n",
    "submission = pd.DataFrame(final_preds, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3 - Follow-up with the error and a hint: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[10], line 51\n",
    "     49 X_train = X_train.dropna(axis=1, how='all')\n",
    "     50 X_val = X_val[X_train.columns]\n",
    "---> 51 df_test = df_test[X_train.columns]\n",
    "     53 # Extract datetime features and drop originals\n",
    "     54 for ds in [X_train, X_val, df_test]:\n",
    "\n",
    "File ~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n",
    "   4111     if is_iterator(key):\n",
    "   4112         key = list(key)\n",
    "-> 4113     indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
    "   4115 # take() does not accept boolean indexers\n",
    "   4116 if getattr(indexer, \"dtype\", None) == bool:\n",
    "\n",
    "File ~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212, in Index._get_indexer_strict(self, key, axis_name)\n",
    "   6209 else:\n",
    "   6210     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
    "-> 6212 self._raise_if_missing(keyarr, indexer, axis_name)\n",
    "   6214 keyarr = self.take(indexer)\n",
    "   6215 if isinstance(key, Index):\n",
    "   6216     # GH 42790 - Preserve name from an Index\n",
    "\n",
    "File ~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264, in Index._raise_if_missing(self, key, indexer, axis_name)\n",
    "   6261     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
    "   6263 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
    "-> 6264 raise KeyError(f\"{not_found} not in index\")\n",
    "\n",
    "KeyError: \"['dropoff_datetime'] not in index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the column doesnt exist in the testing file drop it from the training then too "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18234/18234 - 22s - 1ms/step - loss: 2.3396 - mae: 0.9152 - rmse: 1.5296 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 3/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 4/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 5/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 6/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 7/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 8/100\n",
      "18234/18234 - 20s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 9/100\n",
      "18234/18234 - 21s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 10/100\n",
      "18234/18234 - 20s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 11/100\n",
      "18234/18234 - 20s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "Epoch 12/100\n",
      "18234/18234 - 20s - 1ms/step - loss: 0.4297 - mae: 0.4539 - rmse: 0.6555 - val_loss: 0.4060 - val_mae: 0.4524 - val_rmse: 0.6372\n",
      "\u001b[1m19536/19536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 543us/step\n"
     ]
    }
   ],
   "source": [
    "import random, os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Data loading\n",
    "# Assumes CSV inside each zip has same base name\n",
    "df_train = pd.read_csv('train.zip', compression='zip')\n",
    "df_test = pd.read_csv('test.zip', compression='zip')\n",
    "sample_sub = pd.read_csv('sample_submission.zip', compression='zip')\n",
    "\n",
    "# Infer columns\n",
    "id_col = sample_sub.columns[0]\n",
    "target_columns = sample_sub.columns.tolist()[1:]\n",
    "\n",
    "# Prepare train data\n",
    "df = df_train.copy()\n",
    "\n",
    "# Target encoding for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values)\n",
    "\n",
    "# Features and split\n",
    "X = df.drop(columns=target_columns + [id_col, 'dropoff_datetime'], errors='ignore')  # Drop 'dropoff_datetime'\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Store IDs for submission\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing\n",
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_val = X_val[X_train.columns]\n",
    "df_test = df_test[X_train.columns]\n",
    "\n",
    "# Extract datetime features and drop originals\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['pickup_datetime'] = pd.to_datetime(ds['pickup_datetime'])\n",
    "    ds['hour'] = ds['pickup_datetime'].dt.hour\n",
    "    ds['day'] = ds['pickup_datetime'].dt.day\n",
    "    ds['month'] = ds['pickup_datetime'].dt.month\n",
    "    ds['weekday'] = ds['pickup_datetime'].dt.weekday\n",
    "    ds.drop(columns=['pickup_datetime'], inplace=True, errors='ignore')  # Drop only 'pickup_datetime'\n",
    "\n",
    "# Compute haversine distance\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, (lat1, lon1, lat2, lon2))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a)) * 1000  # meters\n",
    "\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['distance'] = haversine_np(\n",
    "        ds['pickup_latitude'], ds['pickup_longitude'],\n",
    "        ds['dropoff_latitude'], ds['dropoff_longitude']\n",
    "    )\n",
    "    ds.drop(columns=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'], inplace=True)\n",
    "\n",
    "# Identify numeric vs categorical\n",
    "numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# Cap categorical cardinality\n",
    "categorical_cols = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit-transform\n",
    "elems = preprocessor.fit_transform(X_train)\n",
    "X_train_proc = elems\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(df_test)\n",
    "\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "\n",
    "# Build model: large dataset -> deeper\n",
    "units = [min(int(n_features * i), 1024) for i in [2,1,0.5,0.25]]\n",
    "units = [u for u in units if u >= 16]\n",
    "model = Sequential()\n",
    "for u in units:\n",
    "    model.add(Dense(u, activation='relu', input_shape=(n_features,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "# Output layer\n",
    "model.add(Dense(len(target_columns), activation='linear'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_data=(X_val_proc, y_val),\n",
    "    epochs=100, batch_size=64,\n",
    "    callbacks=callbacks, verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1],\n",
    "    'duration_sec': duration\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Predict & submit\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = np.expm1(np.clip(raw_preds, a_min=None, a_max=20))\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1,1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.13052558898926\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 2 Attempts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 41s]\n",
      "val_loss: 0.6225534677505493\n",
      "\n",
      "Best val_loss So Far: 0.6225534677505493\n",
      "Total elapsed time: 00h 01m 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36467/36467 - 95s - 3ms/step - loss: 0.9448 - mae: 0.6957 - rmse: 0.9720 - val_loss: 0.6464 - val_mae: 0.5978 - val_rmse: 0.8040\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'duration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    176\u001b[39m     history = model.fit(\n\u001b[32m    177\u001b[39m         X_train_proc, y_train,\n\u001b[32m    178\u001b[39m         validation_split=\u001b[32m0.2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m         verbose=\u001b[32m2\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m    185\u001b[39m results = {\n\u001b[32m    186\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_loss\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    187\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_rmse\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    188\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidation_loss\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    189\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidation_rmse\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mval_rmse\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mduration_sec\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mduration\u001b[49m\n\u001b[32m    191\u001b[39m }\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mresults.json\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    193\u001b[39m     json.dump(results, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'duration' is not defined"
     ]
    }
   ],
   "source": [
    "import random, os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Data loading\n",
    "# Assumes CSV inside each zip has same base name\n",
    "df_train = pd.read_csv('nyc-taxi-trip-duration/train.zip', compression='zip')\n",
    "df_test = pd.read_csv('nyc-taxi-trip-duration/test.zip', compression='zip')\n",
    "sample_sub = pd.read_csv('nyc-taxi-trip-duration/sample_submission.zip', compression='zip')\n",
    "\n",
    "# Infer columns\n",
    "id_col = sample_sub.columns[0]\n",
    "target_columns = sample_sub.columns.tolist()[1:]\n",
    "\n",
    "# Prepare train data\n",
    "df = df_train.copy()\n",
    "\n",
    "# Target encoding for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values)\n",
    "\n",
    "# Features and split\n",
    "X = df.drop(columns=target_columns + [id_col, 'dropoff_datetime'], errors='ignore')  # Drop 'dropoff_datetime'\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Store IDs for submission\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing\n",
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_val = X_val[X_train.columns]\n",
    "df_test = df_test[X_train.columns]\n",
    "\n",
    "# Extract datetime features and drop originals\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['pickup_datetime'] = pd.to_datetime(ds['pickup_datetime'])\n",
    "    ds['hour'] = ds['pickup_datetime'].dt.hour\n",
    "    ds['day'] = ds['pickup_datetime'].dt.day\n",
    "    ds['month'] = ds['pickup_datetime'].dt.month\n",
    "    ds['weekday'] = ds['pickup_datetime'].dt.weekday\n",
    "    ds.drop(columns=['pickup_datetime'], inplace=True, errors='ignore')  # Drop only 'pickup_datetime'\n",
    "\n",
    "# Compute haversine distance\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, (lat1, lon1, lat2, lon2))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lat2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a)) * 1000  # meters\n",
    "\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['distance'] = haversine_np(\n",
    "        ds['pickup_latitude'], ds['pickup_longitude'],\n",
    "        ds['dropoff_latitude'], ds['dropoff_longitude']\n",
    "    )\n",
    "    ds.drop(columns=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'], inplace=True)\n",
    "\n",
    "# Identify numeric vs categorical\n",
    "numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# Cap categorical cardinality\n",
    "categorical_cols = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit-transform\n",
    "elems = preprocessor.fit_transform(X_train)\n",
    "X_train_proc = elems\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(df_test)\n",
    "\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        model = Sequential()\n",
    "        for _ in range(layers):\n",
    "            model.add(Dense(units, activation='relu', input_shape=(n_features,)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(drop))\n",
    "        model.add(Dense(len(target_columns), activation='linear'))\n",
    "\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "bs = 32  # Example batch size\n",
    "ep = 100  # Example epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1],\n",
    "    'duration_sec': duration\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Predict & submit\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = np.expm1(np.clip(raw_preds, a_min=None, a_max=20))\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1,1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 34m 01s]\n",
      "val_loss: 0.6902359127998352\n",
      "\n",
      "Best val_loss So Far: 0.6206799149513245\n",
      "Total elapsed time: 05h 31m 39s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36467/36467 - 240s - 7ms/step - loss: 0.7677 - mae: 0.6493 - rmse: 0.8762 - val_loss: 0.7298 - val_mae: 0.6046 - val_rmse: 0.8543\n",
      "Epoch 2/100\n",
      "36467/36467 - 197s - 5ms/step - loss: 0.6354 - mae: 0.6053 - rmse: 0.7971 - val_loss: 1.9738 - val_mae: 0.6141 - val_rmse: 1.4049\n",
      "Epoch 3/100\n",
      "36467/36467 - 218s - 6ms/step - loss: 0.6290 - mae: 0.6020 - rmse: 0.7931 - val_loss: 4.9156 - val_mae: 0.6198 - val_rmse: 2.2171\n",
      "Epoch 4/100\n",
      "36467/36467 - 225s - 6ms/step - loss: 0.6275 - mae: 0.6014 - rmse: 0.7922 - val_loss: 11.5599 - val_mae: 0.6222 - val_rmse: 3.4000\n",
      "Epoch 5/100\n",
      "36467/36467 - 216s - 6ms/step - loss: 0.6260 - mae: 0.6005 - rmse: 0.7912 - val_loss: 2.6364 - val_mae: 0.6091 - val_rmse: 1.6237\n",
      "Epoch 6/100\n",
      "36467/36467 - 206s - 6ms/step - loss: 0.6245 - mae: 0.5997 - rmse: 0.7903 - val_loss: 147.3306 - val_mae: 0.6558 - val_rmse: 12.1380\n",
      "Epoch 7/100\n",
      "36467/36467 - 216s - 6ms/step - loss: 0.6234 - mae: 0.5992 - rmse: 0.7895 - val_loss: 1.1764 - val_mae: 0.6129 - val_rmse: 1.0846\n",
      "Epoch 8/100\n",
      "36467/36467 - 218s - 6ms/step - loss: 0.6222 - mae: 0.5985 - rmse: 0.7888 - val_loss: 3.7446 - val_mae: 0.6124 - val_rmse: 1.9351\n",
      "Epoch 9/100\n",
      "36467/36467 - 219s - 6ms/step - loss: 0.6209 - mae: 0.5979 - rmse: 0.7879 - val_loss: 92.2912 - val_mae: 0.6468 - val_rmse: 9.6068\n",
      "Epoch 10/100\n",
      "36467/36467 - 223s - 6ms/step - loss: 0.6197 - mae: 0.5972 - rmse: 0.7872 - val_loss: 53.0394 - val_mae: 0.6542 - val_rmse: 7.2828\n",
      "Epoch 11/100\n",
      "36467/36467 - 226s - 6ms/step - loss: 0.6186 - mae: 0.5966 - rmse: 0.7865 - val_loss: 34.6458 - val_mae: 0.6363 - val_rmse: 5.8861\n",
      "\u001b[1m19536/19536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import random, os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Data loading\n",
    "# Assumes CSV inside each zip has same base name\n",
    "df_train = pd.read_csv('train.zip', compression='zip')\n",
    "df_test = pd.read_csv('test.zip', compression='zip')\n",
    "sample_sub = pd.read_csv('sample_submission.zip', compression='zip')\n",
    "\n",
    "# Infer columns\n",
    "id_col = sample_sub.columns[0]\n",
    "target_columns = sample_sub.columns.tolist()[1:]\n",
    "\n",
    "# Prepare train data\n",
    "df = df_train.copy()\n",
    "\n",
    "# Target encoding for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values)\n",
    "\n",
    "# Features and split\n",
    "X = df.drop(columns=target_columns + [id_col, 'dropoff_datetime'], errors='ignore')  # Drop 'dropoff_datetime'\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Store IDs for submission\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing\n",
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_val = X_val[X_train.columns]\n",
    "df_test = df_test[X_train.columns]\n",
    "\n",
    "# Extract datetime features and drop originals\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['pickup_datetime'] = pd.to_datetime(ds['pickup_datetime'])\n",
    "    ds['hour'] = ds['pickup_datetime'].dt.hour\n",
    "    ds['day'] = ds['pickup_datetime'].dt.day\n",
    "    ds['month'] = ds['pickup_datetime'].dt.month\n",
    "    ds['weekday'] = ds['pickup_datetime'].dt.weekday\n",
    "    ds.drop(columns=['pickup_datetime'], inplace=True, errors='ignore')  # Drop only 'pickup_datetime'\n",
    "\n",
    "# Compute haversine distance\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, (lat1, lon1, lat2, lon2))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lat2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a)) * 1000  # meters\n",
    "\n",
    "for ds in [X_train, X_val, df_test]:\n",
    "    ds['distance'] = haversine_np(\n",
    "        ds['pickup_latitude'], ds['pickup_longitude'],\n",
    "        ds['dropoff_latitude'], ds['dropoff_longitude']\n",
    "    )\n",
    "    ds.drop(columns=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'], inplace=True)\n",
    "\n",
    "# Identify numeric vs categorical\n",
    "numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# Cap categorical cardinality\n",
    "categorical_cols = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit-transform\n",
    "elems = preprocessor.fit_transform(X_train)\n",
    "X_train_proc = elems\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "X_test_proc = preprocessor.transform(df_test)\n",
    "\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        model = Sequential()\n",
    "        for _ in range(layers):\n",
    "            model.add(Dense(units, activation='relu', input_shape=(n_features,)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(drop))\n",
    "        model.add(Dense(len(target_columns), activation='linear'))\n",
    "\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "bs = 32  # Example batch size\n",
    "ep = 100  # Example epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "duration = time.time() - start_time  # Calculate duration\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1],\n",
    "    'duration_sec': duration\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Predict & submit\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = np.expm1(np.clip(raw_preds, a_min=None, a_max=20))\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1,1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414.245115\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
