{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Podcast Listening Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.18842, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 1ms/step - loss: 0.2810 - mean_absolute_error: 0.3736 - root_mean_squared_error: 0.5301 - val_loss: 0.1884 - val_mean_absolute_error: 0.2763 - val_root_mean_squared_error: 0.4341\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.18842\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1925 - mean_absolute_error: 0.2972 - root_mean_squared_error: 0.4388 - val_loss: 0.1917 - val_mean_absolute_error: 0.2767 - val_root_mean_squared_error: 0.4379\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.18842 to 0.18401, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 25s - 1ms/step - loss: 0.1892 - mean_absolute_error: 0.2933 - root_mean_squared_error: 0.4350 - val_loss: 0.1840 - val_mean_absolute_error: 0.2742 - val_root_mean_squared_error: 0.4290\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.18401 to 0.18187, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 26s - 1ms/step - loss: 0.1874 - mean_absolute_error: 0.2919 - root_mean_squared_error: 0.4329 - val_loss: 0.1819 - val_mean_absolute_error: 0.2737 - val_root_mean_squared_error: 0.4265\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.18187 to 0.17826, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 26s - 1ms/step - loss: 0.1859 - mean_absolute_error: 0.2907 - root_mean_squared_error: 0.4312 - val_loss: 0.1783 - val_mean_absolute_error: 0.2726 - val_root_mean_squared_error: 0.4222\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.17826 to 0.17809, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 1ms/step - loss: 0.1855 - mean_absolute_error: 0.2905 - root_mean_squared_error: 0.4307 - val_loss: 0.1781 - val_mean_absolute_error: 0.2724 - val_root_mean_squared_error: 0.4220\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.17809\n",
      "18750/18750 - 28s - 2ms/step - loss: 0.1854 - mean_absolute_error: 0.2906 - root_mean_squared_error: 0.4306 - val_loss: 0.1795 - val_mean_absolute_error: 0.2725 - val_root_mean_squared_error: 0.4236\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.17809\n",
      "18750/18750 - 29s - 2ms/step - loss: 0.1845 - mean_absolute_error: 0.2897 - root_mean_squared_error: 0.4296 - val_loss: 0.1783 - val_mean_absolute_error: 0.2739 - val_root_mean_squared_error: 0.4222\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.17809 to 0.17736, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 32s - 2ms/step - loss: 0.1842 - mean_absolute_error: 0.2896 - root_mean_squared_error: 0.4292 - val_loss: 0.1774 - val_mean_absolute_error: 0.2719 - val_root_mean_squared_error: 0.4211\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.17736\n",
      "18750/18750 - 31s - 2ms/step - loss: 0.1845 - mean_absolute_error: 0.2898 - root_mean_squared_error: 0.4295 - val_loss: 0.1823 - val_mean_absolute_error: 0.2730 - val_root_mean_squared_error: 0.4270\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.17736 to 0.17489, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 30s - 2ms/step - loss: 0.1838 - mean_absolute_error: 0.2895 - root_mean_squared_error: 0.4287 - val_loss: 0.1749 - val_mean_absolute_error: 0.2733 - val_root_mean_squared_error: 0.4182\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.17489\n",
      "18750/18750 - 31s - 2ms/step - loss: 0.1840 - mean_absolute_error: 0.2895 - root_mean_squared_error: 0.4289 - val_loss: 0.1783 - val_mean_absolute_error: 0.2714 - val_root_mean_squared_error: 0.4223\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.17489\n",
      "18750/18750 - 31s - 2ms/step - loss: 0.1838 - mean_absolute_error: 0.2893 - root_mean_squared_error: 0.4287 - val_loss: 0.1801 - val_mean_absolute_error: 0.2722 - val_root_mean_squared_error: 0.4244\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.17489\n",
      "18750/18750 - 33s - 2ms/step - loss: 0.1835 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4284 - val_loss: 0.1782 - val_mean_absolute_error: 0.2724 - val_root_mean_squared_error: 0.4222\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.17489 to 0.17422, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 32s - 2ms/step - loss: 0.1835 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4284 - val_loss: 0.1742 - val_mean_absolute_error: 0.2748 - val_root_mean_squared_error: 0.4174\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss improved from 0.17422 to 0.17187, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 2ms/step - loss: 0.1834 - mean_absolute_error: 0.2892 - root_mean_squared_error: 0.4283 - val_loss: 0.1719 - val_mean_absolute_error: 0.2709 - val_root_mean_squared_error: 0.4146\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17187 to 0.17163, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 1ms/step - loss: 0.1834 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4283 - val_loss: 0.1716 - val_mean_absolute_error: 0.2695 - val_root_mean_squared_error: 0.4143\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.17163\n",
      "18750/18750 - 28s - 2ms/step - loss: 0.1834 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4283 - val_loss: 0.1742 - val_mean_absolute_error: 0.2710 - val_root_mean_squared_error: 0.4174\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.17163\n",
      "18750/18750 - 27s - 1ms/step - loss: 0.1827 - mean_absolute_error: 0.2887 - root_mean_squared_error: 0.4274 - val_loss: 0.1728 - val_mean_absolute_error: 0.2722 - val_root_mean_squared_error: 0.4157\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.17163\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1830 - mean_absolute_error: 0.2889 - root_mean_squared_error: 0.4278 - val_loss: 0.1760 - val_mean_absolute_error: 0.2706 - val_root_mean_squared_error: 0.4195\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss improved from 0.17163 to 0.17067, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 1ms/step - loss: 0.1830 - mean_absolute_error: 0.2890 - root_mean_squared_error: 0.4278 - val_loss: 0.1707 - val_mean_absolute_error: 0.2702 - val_root_mean_squared_error: 0.4131\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss improved from 0.17067 to 0.16936, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 28s - 2ms/step - loss: 0.1835 - mean_absolute_error: 0.2892 - root_mean_squared_error: 0.4283 - val_loss: 0.1694 - val_mean_absolute_error: 0.2700 - val_root_mean_squared_error: 0.4115\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16936\n",
      "18750/18750 - 32s - 2ms/step - loss: 0.1832 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4280 - val_loss: 0.1707 - val_mean_absolute_error: 0.2735 - val_root_mean_squared_error: 0.4131\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16936\n",
      "18750/18750 - 32s - 2ms/step - loss: 0.1829 - mean_absolute_error: 0.2890 - root_mean_squared_error: 0.4277 - val_loss: 0.1724 - val_mean_absolute_error: 0.2715 - val_root_mean_squared_error: 0.4153\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16936\n",
      "18750/18750 - 27s - 1ms/step - loss: 0.1831 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4279 - val_loss: 0.1739 - val_mean_absolute_error: 0.2719 - val_root_mean_squared_error: 0.4171\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16936\n",
      "18750/18750 - 27s - 1ms/step - loss: 0.1829 - mean_absolute_error: 0.2888 - root_mean_squared_error: 0.4276 - val_loss: 0.1748 - val_mean_absolute_error: 0.2725 - val_root_mean_squared_error: 0.4180\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16936\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1830 - mean_absolute_error: 0.2890 - root_mean_squared_error: 0.4278 - val_loss: 0.1754 - val_mean_absolute_error: 0.2719 - val_root_mean_squared_error: 0.4188\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16936\n",
      "18750/18750 - 27s - 1ms/step - loss: 0.1832 - mean_absolute_error: 0.2891 - root_mean_squared_error: 0.4280 - val_loss: 0.1705 - val_mean_absolute_error: 0.2704 - val_root_mean_squared_error: 0.4129\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16936\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1834 - mean_absolute_error: 0.2892 - root_mean_squared_error: 0.4282 - val_loss: 0.1757 - val_mean_absolute_error: 0.2726 - val_root_mean_squared_error: 0.4191\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16936\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1827 - mean_absolute_error: 0.2888 - root_mean_squared_error: 0.4275 - val_loss: 0.1705 - val_mean_absolute_error: 0.2725 - val_root_mean_squared_error: 0.4129\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16936\n",
      "18750/18750 - 27s - 1ms/step - loss: 0.1826 - mean_absolute_error: 0.2886 - root_mean_squared_error: 0.4273 - val_loss: 0.1723 - val_mean_absolute_error: 0.2727 - val_root_mean_squared_error: 0.4151\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16936\n",
      "18750/18750 - 28s - 1ms/step - loss: 0.1824 - mean_absolute_error: 0.2884 - root_mean_squared_error: 0.4271 - val_loss: 0.1708 - val_mean_absolute_error: 0.2732 - val_root_mean_squared_error: 0.4133\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 607us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Data loading\n",
    "train_df = pd.read_csv('playground-series-s5e4/train.csv')\n",
    "test_df = pd.read_csv('playground-series-s5e4/test.csv')\n",
    "\n",
    "id_col = 'id'\n",
    "target_columns = ['Listening_Time_minutes']\n",
    "competition_problem_subtype = 'continuous-regression'\n",
    "\n",
    "# Prepare target\n",
    "Y = train_df[target_columns].astype(float).values\n",
    "if np.all(Y >= 0):\n",
    "    y_enc = np.log1p(Y)\n",
    "else:\n",
    "    y_enc = Y\n",
    "\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Assign train and validation sets (test split provided)\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "train_ids = train_df[id_col]\n",
    "\n",
    "X_val = test_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = test_df[id_col]\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering: drop all-missing columns\n",
    "all_missing = X_train.columns[X_train.isna().all()].tolist()\n",
    "X_train.drop(columns=all_missing, inplace=True)\n",
    "X_val.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# Identify categorical columns and drop high-cardinality ones\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "high_card = [c for c in cat_cols if X_train[c].nunique() > 50]\n",
    "X_train.drop(columns=high_card, inplace=True)\n",
    "X_val.drop(columns=high_card, inplace=True)\n",
    "cat_cols = [c for c in cat_cols if c not in high_card]\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture selection\n",
    "dataset_size, feature_count = X_train_proc.shape\n",
    "if dataset_size < 10000 or feature_count < 100:\n",
    "    layer_sizes = [min(feature_count * 2, 128), min(feature_count, 64)]\n",
    "    dropout_rate = 0.3\n",
    "    use_batchnorm = False\n",
    "else:\n",
    "    raw_sizes = [feature_count * i for i in (2, 1, 0.5, 0.25)]\n",
    "    layer_sizes = [int(s) for s in raw_sizes if s >= 16]\n",
    "    dropout_rate = 0.4\n",
    "    use_batchnorm = True\n",
    "\n",
    "model = Sequential()\n",
    "for idx, size in enumerate(layer_sizes):\n",
    "    if idx == 0:\n",
    "        model.add(Dense(size, activation='relu', input_shape=(feature_count,)))\n",
    "    else:\n",
    "        model.add(Dense(size, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if use_batchnorm:\n",
    "        from tensorflow.keras.layers import BatchNormalization\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "# Output layer for regression\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(), MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "training_duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_accuracy': None,\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': None,\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction on test set\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final_preds = raw_preds\n",
    "if np.all(final_preds >= 0):\n",
    "    final_preds = np.expm1(np.clip(final_preds, a_min=None, a_max=20))\n",
    "final_preds = final_preds.reshape(-1, 1)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame(final_preds, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 11m 52s]\n",
      "val_loss: 0.16736340522766113\n",
      "\n",
      "Best val_loss So Far: 0.16674523055553436\n",
      "Total elapsed time: 02h 04m 20s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 50s - 5ms/step - loss: 0.2956 - mean_absolute_error: 0.3486 - root_mean_squared_error: 0.5437 - val_loss: 0.1814 - val_mean_absolute_error: 0.2766 - val_root_mean_squared_error: 0.4260\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 47s - 5ms/step - loss: 0.2012 - mean_absolute_error: 0.3053 - root_mean_squared_error: 0.4486 - val_loss: 0.1748 - val_mean_absolute_error: 0.2752 - val_root_mean_squared_error: 0.4181\n",
      "Epoch 3/100\n",
      "9375/9375 - 44s - 5ms/step - loss: 0.1912 - mean_absolute_error: 0.2984 - root_mean_squared_error: 0.4372 - val_loss: 0.1774 - val_mean_absolute_error: 0.2703 - val_root_mean_squared_error: 0.4212\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 46s - 5ms/step - loss: 0.1866 - mean_absolute_error: 0.2951 - root_mean_squared_error: 0.4319 - val_loss: 0.1718 - val_mean_absolute_error: 0.2727 - val_root_mean_squared_error: 0.4145\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 43s - 5ms/step - loss: 0.1837 - mean_absolute_error: 0.2929 - root_mean_squared_error: 0.4286 - val_loss: 0.1707 - val_mean_absolute_error: 0.2705 - val_root_mean_squared_error: 0.4132\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 47s - 5ms/step - loss: 0.1811 - mean_absolute_error: 0.2909 - root_mean_squared_error: 0.4255 - val_loss: 0.1691 - val_mean_absolute_error: 0.2720 - val_root_mean_squared_error: 0.4112\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 45s - 5ms/step - loss: 0.1787 - mean_absolute_error: 0.2892 - root_mean_squared_error: 0.4228 - val_loss: 0.1663 - val_mean_absolute_error: 0.2681 - val_root_mean_squared_error: 0.4078\n",
      "Epoch 8/100\n",
      "9375/9375 - 42s - 4ms/step - loss: 0.1764 - mean_absolute_error: 0.2876 - root_mean_squared_error: 0.4201 - val_loss: 0.1715 - val_mean_absolute_error: 0.2728 - val_root_mean_squared_error: 0.4142\n",
      "Epoch 9/100\n",
      "9375/9375 - 43s - 5ms/step - loss: 0.1742 - mean_absolute_error: 0.2861 - root_mean_squared_error: 0.4174 - val_loss: 0.1687 - val_mean_absolute_error: 0.2670 - val_root_mean_squared_error: 0.4107\n",
      "Epoch 10/100\n",
      "9375/9375 - 46s - 5ms/step - loss: 0.1725 - mean_absolute_error: 0.2849 - root_mean_squared_error: 0.4153 - val_loss: 0.1699 - val_mean_absolute_error: 0.2670 - val_root_mean_squared_error: 0.4121\n",
      "Epoch 11/100\n",
      "9375/9375 - 47s - 5ms/step - loss: 0.1705 - mean_absolute_error: 0.2837 - root_mean_squared_error: 0.4130 - val_loss: 0.1689 - val_mean_absolute_error: 0.2662 - val_root_mean_squared_error: 0.4110\n",
      "Epoch 12/100\n",
      "9375/9375 - 44s - 5ms/step - loss: 0.1686 - mean_absolute_error: 0.2825 - root_mean_squared_error: 0.4106 - val_loss: 0.1723 - val_mean_absolute_error: 0.2668 - val_root_mean_squared_error: 0.4151\n",
      "Epoch 13/100\n",
      "9375/9375 - 45s - 5ms/step - loss: 0.1665 - mean_absolute_error: 0.2812 - root_mean_squared_error: 0.4080 - val_loss: 0.1688 - val_mean_absolute_error: 0.2669 - val_root_mean_squared_error: 0.4109\n",
      "Epoch 14/100\n",
      "9375/9375 - 42s - 4ms/step - loss: 0.1648 - mean_absolute_error: 0.2802 - root_mean_squared_error: 0.4059 - val_loss: 0.1736 - val_mean_absolute_error: 0.2674 - val_root_mean_squared_error: 0.4166\n",
      "Epoch 15/100\n",
      "9375/9375 - 46s - 5ms/step - loss: 0.1628 - mean_absolute_error: 0.2790 - root_mean_squared_error: 0.4034 - val_loss: 0.1700 - val_mean_absolute_error: 0.2670 - val_root_mean_squared_error: 0.4123\n",
      "Epoch 16/100\n",
      "9375/9375 - 44s - 5ms/step - loss: 0.1605 - mean_absolute_error: 0.2778 - root_mean_squared_error: 0.4006 - val_loss: 0.1768 - val_mean_absolute_error: 0.2691 - val_root_mean_squared_error: 0.4204\n",
      "Epoch 17/100\n",
      "9375/9375 - 39s - 4ms/step - loss: 0.1584 - mean_absolute_error: 0.2766 - root_mean_squared_error: 0.3980 - val_loss: 0.1802 - val_mean_absolute_error: 0.2686 - val_root_mean_squared_error: 0.4245\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.16944, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 98s - 5ms/step - loss: 0.1934 - mean_absolute_error: 0.3048 - root_mean_squared_error: 0.4397 - val_loss: 0.1694 - val_mean_absolute_error: 0.2694 - val_root_mean_squared_error: 0.4116\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.16944 to 0.16883, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 88s - 5ms/step - loss: 0.1893 - mean_absolute_error: 0.3017 - root_mean_squared_error: 0.4351 - val_loss: 0.1688 - val_mean_absolute_error: 0.2707 - val_root_mean_squared_error: 0.4109\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16883\n",
      "18750/18750 - 92s - 5ms/step - loss: 0.1866 - mean_absolute_error: 0.2997 - root_mean_squared_error: 0.4320 - val_loss: 0.2797 - val_mean_absolute_error: 0.2796 - val_root_mean_squared_error: 0.5288\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16883\n",
      "18750/18750 - 80s - 4ms/step - loss: 0.1841 - mean_absolute_error: 0.2977 - root_mean_squared_error: 0.4290 - val_loss: 1.5239 - val_mean_absolute_error: 0.2862 - val_root_mean_squared_error: 1.2344\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16883\n",
      "18750/18750 - 88s - 5ms/step - loss: 0.1822 - mean_absolute_error: 0.2963 - root_mean_squared_error: 0.4268 - val_loss: 0.1794 - val_mean_absolute_error: 0.2697 - val_root_mean_squared_error: 0.4236\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16883\n",
      "18750/18750 - 91s - 5ms/step - loss: 0.1803 - mean_absolute_error: 0.2950 - root_mean_squared_error: 0.4247 - val_loss: 45.5384 - val_mean_absolute_error: 0.3632 - val_root_mean_squared_error: 6.7482\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16883\n",
      "18750/18750 - 87s - 5ms/step - loss: 0.1790 - mean_absolute_error: 0.2941 - root_mean_squared_error: 0.4230 - val_loss: 1.2001 - val_mean_absolute_error: 0.2856 - val_root_mean_squared_error: 1.0955\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16883\n",
      "18750/18750 - 84s - 4ms/step - loss: 0.1771 - mean_absolute_error: 0.2929 - root_mean_squared_error: 0.4209 - val_loss: 4.1416 - val_mean_absolute_error: 0.3093 - val_root_mean_squared_error: 2.0351\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16883\n",
      "18750/18750 - 92s - 5ms/step - loss: 0.1757 - mean_absolute_error: 0.2920 - root_mean_squared_error: 0.4192 - val_loss: 5469.0166 - val_mean_absolute_error: 0.9799 - val_root_mean_squared_error: 73.9528\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16883\n",
      "18750/18750 - 87s - 5ms/step - loss: 0.1739 - mean_absolute_error: 0.2910 - root_mean_squared_error: 0.4171 - val_loss: 95687.1562 - val_mean_absolute_error: 7.6834 - val_root_mean_squared_error: 309.3334\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16883\n",
      "18750/18750 - 81s - 4ms/step - loss: 0.1729 - mean_absolute_error: 0.2903 - root_mean_squared_error: 0.4158 - val_loss: 22519.9121 - val_mean_absolute_error: 1.9157 - val_root_mean_squared_error: 150.0664\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16883\n",
      "18750/18750 - 88s - 5ms/step - loss: 0.1715 - mean_absolute_error: 0.2896 - root_mean_squared_error: 0.4141 - val_loss: 91.4800 - val_mean_absolute_error: 0.4423 - val_root_mean_squared_error: 9.5645\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Data loading\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "id_col = 'id'\n",
    "target_columns = ['Listening_Time_minutes']\n",
    "competition_problem_subtype = 'continuous-regression'\n",
    "\n",
    "# Prepare target\n",
    "Y = train_df[target_columns].astype(float).values\n",
    "if np.all(Y >= 0):\n",
    "    y_enc = np.log1p(Y)\n",
    "else:\n",
    "    y_enc = Y\n",
    "\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Assign train and validation sets (test split provided)\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "train_ids = train_df[id_col]\n",
    "\n",
    "X_val = test_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = test_df[id_col]\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering: drop all-missing columns\n",
    "all_missing = X_train.columns[X_train.isna().all()].tolist()\n",
    "X_train.drop(columns=all_missing, inplace=True)\n",
    "X_val.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# Identify categorical columns and drop high-cardinality ones\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "high_card = [c for c in cat_cols if X_train[c].nunique() > 50]\n",
    "X_train.drop(columns=high_card, inplace=True)\n",
    "X_val.drop(columns=high_card, inplace=True)\n",
    "cat_cols = [c for c in cat_cols if c not in high_card]\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture selection\n",
    "dataset_size, feature_count = X_train_proc.shape\n",
    "if dataset_size < 10000 or feature_count < 100:\n",
    "    layer_sizes = [min(feature_count * 2, 128), min(feature_count, 64)]\n",
    "    dropout_rate = 0.3\n",
    "    use_batchnorm = False\n",
    "else:\n",
    "    raw_sizes = [feature_count * i for i in (2, 1, 0.5, 0.25)]\n",
    "    layer_sizes = [int(s) for s in raw_sizes if s >= 16]\n",
    "    dropout_rate = 0.4\n",
    "    use_batchnorm = True\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = feature_count\n",
    "\n",
    "# HyperModel\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        model = Sequential()\n",
    "        for idx in range(layers):\n",
    "            if idx == 0:\n",
    "                model.add(Dense(units, activation='relu', input_shape=(n_features,)))\n",
    "            else:\n",
    "                model.add(Dense(units, activation='relu'))\n",
    "            model.add(Dropout(drop))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "        return model\n",
    "\n",
    "# Tuner\n",
    "bs = 64  # batch size\n",
    "ep = 100  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=bs, epochs=ep,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100, batch_size=bs,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "training_duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_accuracy': None,\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': None,\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction on test set\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final_preds = raw_preds\n",
    "if np.all(final_preds >= 0):\n",
    "    final_preds = np.expm1(np.clip(final_preds, a_min=None, a_max=20))\n",
    "final_preds = final_preds.reshape(-1, 1)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame(final_preds, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468.453455\n"
     ]
    }
   ],
   "source": [
    "print(training_duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
