{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Optimal Fertilizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - 3 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt - 1 Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    131\u001b[39m start=time.time()\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_val_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     history=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     history=model.fit(\n\u001b[32m    140\u001b[39m         X_train_proc,y_train,validation_split=\u001b[32m0.2\u001b[39m,\n\u001b[32m    141\u001b[39m         epochs=\u001b[32m100\u001b[39m,batch_size=\u001b[32m128\u001b[39m,callbacks=cbs,verbose=\u001b[32m2\u001b[39m\n\u001b[32m    142\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/optree/ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: None values not supported."
     ]
    }
   ],
   "source": [
    "# reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import json\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "\n",
    "# load data\n",
    "data_files = [\"playground-series-s5e6/sample_submission.csv\",\"playground-series-s5e6/test.csv\",\"playground-series-s5e6/train.csv\"]\n",
    "train_dfs = []\n",
    "df_test = None\n",
    "for f in data_files:\n",
    "    if f.endswith('train.csv'):\n",
    "        train_dfs.append(pd.read_csv(f))\n",
    "    elif f.endswith('test.csv'):\n",
    "        df_test = pd.read_csv(f)\n",
    "# infer id and target\n",
    "id_col = 'id'\n",
    "target_columns = ['Fertilizer Name']\n",
    "# prepare train\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# multi-label encoding\n",
    "col = target_columns[0]\n",
    "df[col] = df[col].astype(str).str.split()\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_enc = mlb.fit_transform(df[col])\n",
    "classes_ = mlb.classes_\n",
    "\n",
    "# features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# split\n",
    "if df_test is None:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_enc, test_size=0.2, random_state=42\n",
    "    )\n",
    "    test_ids = None\n",
    "else:\n",
    "    X_train = X\n",
    "    y_train = y_enc\n",
    "    test_ids = df_test[id_col]\n",
    "    X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "    y_val = None\n",
    "\n",
    "# drop all-missing cols\n",
    "def drop_missing(df1, df2=None):\n",
    "    df1 = df1.dropna(axis=1, how='all')\n",
    "    if df2 is not None:\n",
    "        df2 = df2[df1.columns]\n",
    "        return df1, df2\n",
    "    return df1\n",
    "\n",
    "if df_test is not None:\n",
    "    X_train, X_val = drop_missing(X_train, X_val)\n",
    "else:\n",
    "    X_train = drop_missing(X_train)\n",
    "\n",
    "# drop high-cardinality cats\n",
    "cat_cols_full = X_train.select_dtypes(include=['object','category']).columns\n",
    "high_card = [c for c in cat_cols_full if X_train[c].nunique()>50]\n",
    "X_train = X_train.drop(columns=high_card)\n",
    "if df_test is not None:\n",
    "    X_val = X_val.drop(columns=high_card)\n",
    "\n",
    "# preprocessing pipeline\n",
    "num_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_pipe = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val) if df_test is not None else None\n",
    "\n",
    "# model\n",
    "n_features = X_train_proc.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "n_samples = X_train_proc.shape[0]\n",
    "# layer sizes\n",
    "if n_samples<10000 or n_features<100:\n",
    "    layer_sizes=[min(n_features*2,128),min(n_features,64)]\n",
    "    drop_rate=0.3\n",
    "else:\n",
    "    sizes=[min(n_features*2,1024),min(n_features,1024),\n",
    "           min(int(n_features*0.5),1024),min(int(n_features*0.25),1024)]\n",
    "    layer_sizes=[s for s in sizes if s>=16]\n",
    "    drop_rate=0.4\n",
    "# build\n",
    "inputs=Input(shape=(n_features,))\n",
    "x=inputs\n",
    "for s in layer_sizes:\n",
    "    x=Dense(s,activation='relu')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Dropout(drop_rate)(x)\n",
    "outputs=Dense(n_classes,activation='sigmoid')(x)\n",
    "model=Model(inputs,outputs)\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer='adam',loss='binary_crossentropy',\n",
    "    metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
    ")\n",
    "# callbacks\n",
    "cbs=[\n",
    "    EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5',monitor='val_loss',save_best_only=True)\n",
    "]\n",
    "# train\n",
    "start=time.time()\n",
    "if X_val_proc is not None:\n",
    "    history=model.fit(\n",
    "        X_train_proc,y_train,\n",
    "        validation_data=(X_val_proc,y_val),\n",
    "        epochs=100,batch_size=128,callbacks=cbs,verbose=2\n",
    "    )\n",
    "else:\n",
    "    history=model.fit(\n",
    "        X_train_proc,y_train,validation_split=0.2,\n",
    "        epochs=100,batch_size=128,callbacks=cbs,verbose=2\n",
    "    )\n",
    "end=time.time()\n",
    "# log results\n",
    "res={\n",
    "    'training_accuracy':history.history['accuracy'][-1],\n",
    "    'training_loss':history.history['loss'][-1],\n",
    "    'validation_accuracy':history.history.get('val_accuracy', [None])[-1],\n",
    "    'validation_loss':history.history.get('val_loss',[None])[-1]\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(res,f)\n",
    "# predict\n",
    "X_test_proc = X_val_proc if X_val_proc is not None else preprocessor.transform(X)\n",
    "raw=model.predict(X_test_proc)\n",
    "final=(raw>0.5).astype(int)\n",
    "if final.ndim==1:final=final.reshape(-1,1)\n",
    "sub=pd.DataFrame(final,columns=classes_)\n",
    "sub.insert(0,id_col,test_ids.reset_index(drop=True))\n",
    "sub.to_csv('submission_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing time: 0.95s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,012</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">990</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │         \u001b[38;5;34m1,012\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │           \u001b[38;5;34m990\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,163</span> (8.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,163\u001b[0m (8.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,163</span> (8.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,163\u001b[0m (8.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    114\u001b[39m callbacks = [\n\u001b[32m    115\u001b[39m     EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m),\n\u001b[32m    116\u001b[39m     ModelCheckpoint(\u001b[33m'\u001b[39m\u001b[33mbest_model.h5\u001b[39m\u001b[33m'\u001b[39m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m    117\u001b[39m ]\n\u001b[32m    118\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m    126\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m training_duration = time.time() - start_time\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# 8. Logging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mf1_score_metric\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf1_score_metric\u001b[39m(y_true, y_pred):\n\u001b[32m     85\u001b[39m     y_pred_round = K.round(y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     tp = K.sum(K.cast(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_round\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m'\u001b[39m), axis=\u001b[32m0\u001b[39m)\n\u001b[32m     87\u001b[39m     pp = K.sum(K.cast(y_pred_round, \u001b[33m'\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m'\u001b[39m), axis=\u001b[32m0\u001b[39m)\n\u001b[32m     88\u001b[39m     ap = K.sum(K.cast(y_true, \u001b[33m'\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m'\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# 1. Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 2. Data Loading\n",
    "train_df = pd.read_csv('playground-series-s5e6/train.csv')\n",
    "test_df = pd.read_csv('playground-series-s5e6/test.csv')\n",
    "\n",
    "# Infer id and target columns\n",
    "id_col = 'id'\n",
    "target_col = 'Fertilizer Name'\n",
    "\n",
    "# 3. Target Encoding for multi-label classification\n",
    "def parse_labels(x):\n",
    "    return x.split() if isinstance(x, str) else []\n",
    "\n",
    "y_raw = train_df[target_col].astype(str).apply(parse_labels)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_enc = mlb.fit_transform(y_raw)\n",
    "classes_ = mlb.classes_\n",
    "\n",
    "# 4. Feature matrix\n",
    "drop_cols = [target_col]\n",
    "X = train_df.drop(columns=drop_cols + [id_col], errors='ignore')\n",
    "X_test = test_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "\n",
    "# 5. Drop columns with all missing values\n",
    "all_missing = [col for col in X.columns if X[col].isna().all()]\n",
    "X.drop(columns=all_missing, inplace=True)\n",
    "X_test.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "# Identify categorical and numeric\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Remove high-cardinality cat\n",
    "cat_keep = [col for col in categorical_features if X[col].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', cat_transformer, cat_keep)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "t0 = time.time()\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "print(f'Preprocessing time: {time.time() - t0:.2f}s')\n",
    "\n",
    "# 6. Model Architecture (multi-label, shallow for n_features<100)\n",
    "n_samples, n_features = X_proc.shape\n",
    "n_classes = len(classes_)\n",
    "\n",
    "# Custom F1 metric\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    y_pred_round = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true * y_pred_round, 'float'), axis=0)\n",
    "    pp = K.sum(K.cast(y_pred_round, 'float'), axis=0)\n",
    "    ap = K.sum(K.cast(y_true, 'float'), axis=0)\n",
    "    precision = tp / (pp + K.epsilon())\n",
    "    recall = tp / (ap + K.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    return K.mean(f1)\n",
    "\n",
    "# Build model\n",
    "inputs = tf.keras.Input(shape=(n_features,))\n",
    "# Layer 1\n",
    "x = tf.keras.layers.Dense(min(n_features*2, 128), activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "# Layer 2\n",
    "x = tf.keras.layers.Dense(min(n_features, 64), activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "# Output\n",
    "y_pred = tf.keras.layers.Dense(n_classes, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=y_pred)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', Precision(), Recall(), f1_score_metric]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 7. Callbacks & Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_proc, y_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "training_duration = time.time() - start_time\n",
    "\n",
    "# 8. Logging\n",
    "results = {\n",
    "    'training_accuracy': float(history.history['accuracy'][-1]),\n",
    "    'training_loss': float(history.history['loss'][-1]),\n",
    "    'validation_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "    'validation_loss': float(history.history['val_loss'][-1]),\n",
    "    'training_time_sec': training_duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 9. Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "binary_preds = (raw_preds > 0.5).astype(int)\n",
    "labels = mlb.inverse_transform(binary_preds)\n",
    "# Join labels\n",
    "submission_list = [' '.join(label_set) if len(label_set)>0 else '' for label_set in labels]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    id_col: test_df[id_col].values,\n",
    "    target_col: submission_list\n",
    "})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt - 3\n",
    "\n",
    "## Required to give a small hint: \"It is a mutliclass classification where you need to predict top-3 fertilizer names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 11s - 2ms/step - loss: 0.4298 - precision: 0.1410 - recall: 0.0172 - val_loss: 0.4097 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4108 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4096 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4100 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4096 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4094 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4092 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 10s - 2ms/step - loss: 0.4091 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4091 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 8s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/100\n",
      "4688/4688 - 8s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 8s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/100\n",
      "4688/4688 - 8s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 31/100\n",
      "4688/4688 - 8s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 32/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 33/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 38/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 39/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 40/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 41/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 42/100\n",
      "4688/4688 - 9s - 2ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 621us/step\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "files = [f for f in [\"playground-series-s5e6/sample_submission.csv\",\"playground-series-s5e6/test.csv\",\"playground-series-s5e6/train.csv\"] if f != 'playground-series-s5e6/sample_submission.csv']\n",
    "train_dfs = []\n",
    "df_test = None\n",
    "for f in files:\n",
    "    if f.endswith('test.csv'):\n",
    "        df_test = pd.read_csv(f)\n",
    "    else:\n",
    "        train_dfs.append(pd.read_csv(f))\n",
    "\n",
    "# Infer id and target\n",
    "id_col = 'id'\n",
    "target_columns = ['Fertilizer Name']\n",
    "\n",
    "# Combine training\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# Encode targets for multi-label\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# split space-delimited labels\n",
    "y_list = df['Fertilizer Name'].astype(str).str.split(' ')\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_enc = mlb.fit_transform(y_list)\n",
    "classes_ = mlb.classes_\n",
    "\n",
    "# Prepare features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# If df_test exists, set train and test\n",
    "if df_test is not None:\n",
    "    X_train = X.copy()\n",
    "    y_train = y_enc\n",
    "    train_ids = df[id_col]\n",
    "    test_ids = df_test[id_col]\n",
    "    X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "    y_val = None\n",
    "else:\n",
    "    strat = None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_enc,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=strat\n",
    "    )\n",
    "    train_ids = X_train[id_col] if id_col in X_train else None\n",
    "    test_ids = X_val[id_col] if id_col in X_val else None\n",
    "\n",
    "# Feature engineering\n",
    "# Drop cols all missing\n",
    "drop_all_missing = [c for c in X_train.columns if X_train[c].isna().all()]\n",
    "X_train.drop(columns=drop_all_missing, inplace=True)\n",
    "X_val.drop(columns=drop_all_missing, inplace=True)\n",
    "if df_test is not None:\n",
    "    X_val = X_val.copy()\n",
    "\n",
    "# Identify categorical vars\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Drop high-cardinality\n",
    "low_card_cats = [c for c in cat_cols if X_train[c].nunique() <= 50]\n",
    "high_card = set(cat_cols) - set(low_card_cats)\n",
    "X_train.drop(columns=list(high_card), inplace=True)\n",
    "X_val.drop(columns=list(high_card), inplace=True)\n",
    "\n",
    "# Numeric columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, low_card_cats)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_classes = len(classes_)\n",
    "\n",
    "# Build layers sizes\n",
    "def get_layer_sizes(n_feat):\n",
    "    sizes = []\n",
    "    for i in [2,1,0.5,0.25]:\n",
    "        sz = int(min(n_feat * i, 1024))\n",
    "        if sz >= 16:\n",
    "            sizes.append(sz)\n",
    "    return sizes\n",
    "\n",
    "layer_sizes = get_layer_sizes(n_features)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# Input layer implicit\n",
    "for sz in layer_sizes:\n",
    "    model.add(Dense(sz, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "# Output layer\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "timestamp = int(time.time())\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(f'model_{timestamp}.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "t0 = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - t0\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'training_accuracy': history.history.get('recall')[-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history.get('val_recall')[-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Predictions\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "# Use top 3 predictions\n",
    "top_k = 3\n",
    "preds = []\n",
    "for row in raw_preds:\n",
    "    idxs = np.argsort(-row)[:top_k]\n",
    "    names = classes_[idxs]\n",
    "    preds.append(' '.join(names))\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({id_col: test_ids.reset_index(drop=True), 'Fertilizer Name': preds})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 20m 08s]\n",
      "val_loss: 0.4092257022857666\n",
      "\n",
      "Best val_loss So Far: 0.4089970588684082\n",
      "Total elapsed time: 04h 43m 15s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 83s - 4ms/step - loss: 0.4167 - precision: 0.1445 - recall: 0.0033 - val_loss: 0.4096 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 76s - 4ms/step - loss: 0.4098 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 71s - 4ms/step - loss: 0.4096 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 72s - 4ms/step - loss: 0.4095 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 80s - 4ms/step - loss: 0.4094 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 80s - 4ms/step - loss: 0.4093 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 77s - 4ms/step - loss: 0.4092 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 79s - 4ms/step - loss: 0.4092 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 83s - 4ms/step - loss: 0.4091 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4091 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4091 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 77s - 4ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4092 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 80s - 4ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 77s - 4ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 16/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4090 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 17/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 18/100\n",
      "18750/18750 - 79s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 76s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 20/100\n",
      "18750/18750 - 79s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 21/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 22/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 23/100\n",
      "18750/18750 - 73s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 24/100\n",
      "18750/18750 - 74s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 25/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 76s - 4ms/step - loss: 0.4089 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 - 79s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 28/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 29/100\n",
      "18750/18750 - 76s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 30/100\n",
      "18750/18750 - 80s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 31/100\n",
      "18750/18750 - 80s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 32/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 33/100\n",
      "18750/18750 - 74s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 34/100\n",
      "18750/18750 - 70s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 41/100\n",
      "18750/18750 - 66s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 42/100\n",
      "18750/18750 - 78s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 43/100\n",
      "18750/18750 - 74s - 4ms/step - loss: 0.4088 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 44/100\n",
      "18750/18750 - 72s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 45/100\n",
      "18750/18750 - 74s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 46/100\n",
      "18750/18750 - 74s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 47/100\n",
      "18750/18750 - 75s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 48/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4089 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 49/100\n",
      "18750/18750 - 77s - 4ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 50/100\n",
      "18750/18750 - 66s - 3ms/step - loss: 0.4087 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.4090 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "files = [f for f in [\"sample_submission.csv\",\"test.csv\",\"train.csv\"] if f != 'sample_submission.csv']\n",
    "train_dfs = []\n",
    "df_test = None\n",
    "for f in files:\n",
    "    if f.endswith('test.csv'):\n",
    "        df_test = pd.read_csv(f)\n",
    "    else:\n",
    "        train_dfs.append(pd.read_csv(f))\n",
    "\n",
    "# Infer id and target\n",
    "id_col = 'id'\n",
    "target_columns = ['Fertilizer Name']\n",
    "\n",
    "# Combine training\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# Encode targets for multi-label\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# split space-delimited labels\n",
    "y_list = df['Fertilizer Name'].astype(str).str.split(' ')\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_enc = mlb.fit_transform(y_list)\n",
    "classes_ = mlb.classes_\n",
    "\n",
    "# Prepare features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# If df_test exists, set train and test\n",
    "if df_test is not None:\n",
    "    X_train = X.copy()\n",
    "    y_train = y_enc\n",
    "    train_ids = df[id_col]\n",
    "    test_ids = df_test[id_col]\n",
    "    X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "    y_val = None\n",
    "else:\n",
    "    strat = None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_enc,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=strat\n",
    "    )\n",
    "    train_ids = X_train[id_col] if id_col in X_train else None\n",
    "    test_ids = X_val[id_col] if id_col in X_val else None\n",
    "\n",
    "# Feature engineering\n",
    "# Drop cols all missing\n",
    "drop_all_missing = [c for c in X_train.columns if X_train[c].isna().all()]\n",
    "X_train.drop(columns=drop_all_missing, inplace=True)\n",
    "X_val.drop(columns=drop_all_missing, inplace=True)\n",
    "if df_test is not None:\n",
    "    X_val = X_val.copy()\n",
    "\n",
    "# Identify categorical vars\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# Drop high-cardinality\n",
    "low_card_cats = [c for c in cat_cols if X_train[c].nunique() <= 50]\n",
    "high_card = set(cat_cols) - set(low_card_cats)\n",
    "X_train.drop(columns=list(high_card), inplace=True)\n",
    "X_val.drop(columns=list(high_card), inplace=True)\n",
    "\n",
    "# Numeric columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, low_card_cats)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_classes = len(classes_)\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        x = Dense(n_classes, activation='sigmoid')(x)  # Output layer\n",
    "        model = Model(inputs, x)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['precision', 'recall'])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['precision', 'recall'])\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        initial_epoch = 33,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        initial_epoch = 33,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "duration = time.time() - start_time  # Calculate duration\n",
    "\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'training_accuracy': history.history.get('recall')[-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history.get('val_recall')[-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json','w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Predictions\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "# Use top 3 predictions\n",
    "top_k = 3\n",
    "preds = []\n",
    "for row in raw_preds:\n",
    "    idxs = np.argsort(-row)[:top_k]\n",
    "    names = classes_[idxs]\n",
    "    preds.append(' '.join(names))\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({id_col: test_ids.reset_index(drop=True), 'Fertilizer Name': preds})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795.496926\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
