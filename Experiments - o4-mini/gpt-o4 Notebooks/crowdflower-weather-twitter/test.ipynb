{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partly Sunny with a Chance of Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 22:55:49.475977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752101749.499882  934364 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752101749.507159  934364 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752101749.533052  934364 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101749.533070  934364 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101749.533072  934364 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101749.533075  934364 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 22:55:49.542003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-07-09 22:55:53.366731: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69315, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488/488 - 2s - 5ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.69315\n",
      "488/488 - 1s - 2ms/step - accuracy: 4.3300e-04 - loss: 0.6931 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 1.2829e-04 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m1318/1318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')\n",
    "sub_example = pd.read_csv('sampleSubmission.csv.zip', nrows=1)\n",
    "id_col = sub_example.columns[0]\n",
    "target_columns = list(sub_example.columns[1:])\n",
    "\n",
    "# Prepare X and y\n",
    "# Drop rows/columns with all missing in train\n",
    "train_df = train_df.dropna(axis=1, how='all')\n",
    "\n",
    "y = train_df[target_columns].values\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Prepare test features and ids\n",
    "X_test = test_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = test_df[id_col].reset_index(drop=True)\n",
    "\n",
    "# Feature engineering: detect numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype=='object' and X[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model architecture decision\n",
    "d, m = X_proc.shape[1], len(target_columns)\n",
    "n_samples = X_proc.shape[0]\n",
    "\n",
    "model = Sequential()\n",
    "if n_samples < 10000 or d < 100:\n",
    "    # small dataset\n",
    "    units1 = min(d * 2, 128)\n",
    "    units2 = min(d, 64)\n",
    "    model.add(Dense(units1, activation='relu', input_shape=(d,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units2, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "else:\n",
    "    # larger dataset (unused here but provided)\n",
    "    layers = [min(d * i, 1024) for i in (2, 1, 0.5, 0.25)]\n",
    "    layers = [u for u in layers if u >= 16]\n",
    "    model.add(Dense(layers[0], activation='relu', input_shape=(d,)))\n",
    "    for u in layers[1:]:\n",
    "        model.add(Dense(u, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "# Output layer for multi-label classification\n",
    "model.add(Dense(m, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_proc, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "res = {\n",
    "    'training_accuracy': history.history['accuracy'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_accuracy'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "# Predictions & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = (raw_preds > 0.5).astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids)\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 03m 06s]\n",
      "val_loss: 0.3097052574157715\n",
      "\n",
      "Best val_loss So Far: 0.3097052574157715\n",
      "Total elapsed time: 00h 32m 10s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 15s - 15ms/step - accuracy: 0.3206 - loss: 0.5541 - precision: 0.8844 - recall: 0.1655 - val_accuracy: 0.3242 - val_loss: 0.4544 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.4044 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3681 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3492 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3349 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3273 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3212 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 4s - 5ms/step - accuracy: 0.3210 - loss: 0.3179 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3152 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3137 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3124 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3117 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3110 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3108 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3103 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3103 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3100 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 4s - 5ms/step - accuracy: 0.3210 - loss: 0.3100 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3098 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 4s - 5ms/step - accuracy: 0.3210 - loss: 0.3099 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3098 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 6s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 20/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 24/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 25/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 26/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 27/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 28/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 29/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 30/100\n",
      "975/975 - 5s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 31/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 32/100\n",
      "975/975 - 6s - 6ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "Epoch 33/100\n",
      "975/975 - 5s - 5ms/step - accuracy: 0.3210 - loss: 0.3098 - precision: 0.8844 - recall: 0.1656 - val_accuracy: 0.3242 - val_loss: 0.3097 - val_precision: 0.8863 - val_recall: 0.1654\n",
      "\u001b[1m1310/1318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 01:00:23.888568: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1318/1318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')\n",
    "sub_example = pd.read_csv('sampleSubmission.csv.zip', nrows=1)\n",
    "id_col = sub_example.columns[0]\n",
    "target_columns = list(sub_example.columns[1:])\n",
    "\n",
    "# Prepare X and y\n",
    "# Drop rows/columns with all missing in train\n",
    "train_df = train_df.dropna(axis=1, how='all')\n",
    "\n",
    "y = train_df[target_columns].values\n",
    "X = train_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Prepare test features and ids\n",
    "X_test = test_df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = test_df[id_col].reset_index(drop=True)\n",
    "\n",
    "# Feature engineering: detect numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype=='object' and X[c].nunique() <= 50]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model architecture decision\n",
    "d, m = X_proc.shape[1], len(target_columns)\n",
    "n_samples = X_proc.shape[0]\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Define early stopping and checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = X_proc.shape[1]\n",
    "\n",
    "# HyperModel\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        x = Dense(m, activation='sigmoid')(x)  # Output layer for multi-label classification\n",
    "        model = Model(inputs, x)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "        return model\n",
    "\n",
    "# Tuner\n",
    "bs = 64  # batch size\n",
    "ep = 50  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y is not None:\n",
    "    tuner.search(\n",
    "        X_proc, y,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_proc, y,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "if y is not None:\n",
    "    history = model.fit(\n",
    "        X_proc, y,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_proc, y,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "duration = time.time() - start_time  # Calculate duration\n",
    "\n",
    "\n",
    "# Save results\n",
    "res = {\n",
    "    'training_accuracy': history.history['accuracy'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_accuracy'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(res, f)\n",
    "\n",
    "# Predictions & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "final = (raw_preds > 0.5).astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids)\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.957296\n"
     ]
    }
   ],
   "source": [
    "print(duration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
