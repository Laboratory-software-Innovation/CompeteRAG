{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Playground Series - Jun 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 13:21:31.132557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752153691.157185  966351 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752153691.165268  966351 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752153691.187270  966351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752153691.187298  966351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752153691.187300  966351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752153691.187302  966351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 13:21:31.194504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing numeric features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 13:21:56.725679: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,449</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m10,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m)             │        \u001b[38;5;34m10,449\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,521</span> (146.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,521\u001b[0m (146.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,521</span> (146.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,521\u001b[0m (146.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 14s - 2ms/step - loss: 0.7957 - mae: 0.7047 - rmse: 0.8920 - val_loss: 0.6824 - val_mae: 0.6488 - val_rmse: 0.8261\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 13s - 2ms/step - loss: 0.7670 - mae: 0.6911 - rmse: 0.8758 - val_loss: 0.6699 - val_mae: 0.6427 - val_rmse: 0.8184\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 13s - 2ms/step - loss: 0.7582 - mae: 0.6859 - rmse: 0.8707 - val_loss: 0.6641 - val_mae: 0.6397 - val_rmse: 0.8149\n",
      "Epoch 4/100\n",
      "6250/6250 - 13s - 2ms/step - loss: 0.7528 - mae: 0.6827 - rmse: 0.8677 - val_loss: 0.6672 - val_mae: 0.6415 - val_rmse: 0.8169\n",
      "Epoch 5/100\n",
      "6250/6250 - 13s - 2ms/step - loss: 0.7499 - mae: 0.6807 - rmse: 0.8660 - val_loss: 0.6678 - val_mae: 0.6422 - val_rmse: 0.8172\n",
      "Epoch 6/100\n",
      "6250/6250 - 14s - 2ms/step - loss: 0.7479 - mae: 0.6794 - rmse: 0.8648 - val_loss: 0.6690 - val_mae: 0.6431 - val_rmse: 0.8179\n",
      "Epoch 7/100\n",
      "6250/6250 - 13s - 2ms/step - loss: 0.7462 - mae: 0.6783 - rmse: 0.8638 - val_loss: 0.6702 - val_mae: 0.6436 - val_rmse: 0.8187\n",
      "Epoch 8/100\n",
      "6250/6250 - 13s - 2ms/step - loss: 0.7446 - mae: 0.6775 - rmse: 0.8629 - val_loss: 0.6712 - val_mae: 0.6447 - val_rmse: 0.8192\n",
      "Training completed in 105.77 seconds.\n",
      "Reconstructing and imputing missing values...\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 639us/step\n",
      "Generating submission file...\n",
      "Saving results...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf_import_error = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "except ImportError:\n",
    "    tf_import_error = True\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if not tf_import_error:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('data.csv.zip')\n",
    "df_test = pd.read_csv('sample_submission.csv.zip')\n",
    "\n",
    "# Infer identifiers\n",
    "id_col = 'row-col'\n",
    "target_col = 'value'\n",
    "\n",
    "# Identify continuous features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training numeric data\n",
    "print(\"Preprocessing numeric features...\")\n",
    "X_train_num = num_pipeline.fit_transform(df[num_cols])\n",
    "\n",
    "# Autoencoder model definition\n",
    "n_features = X_train_num.shape[1]\n",
    "input_layer = keras.Input(shape=(n_features,))\n",
    "# Since n_features < 100, use small architecture\n",
    "dense1 = layers.Dense(min(n_features*2,128), activation='relu')(input_layer)\n",
    "drop1 = layers.Dropout(0.3)(dense1)\n",
    "dense2 = layers.Dense(min(n_features,64), activation='relu')(drop1)\n",
    "drop2 = layers.Dropout(0.3)(dense2)\n",
    "dense3 = layers.Dense(min(n_features*2,128), activation='relu')(drop2)\n",
    "drop3 = layers.Dropout(0.3)(dense3)\n",
    "output_layer = layers.Dense(n_features, activation='linear')(drop3)\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "             tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "callbacks = [early_stop, checkpoint]\n",
    "\n",
    "# Training\n",
    "t_start = time.time()\n",
    "history = model.fit(\n",
    "    X_train_num, X_train_num,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "t_duration = time.time() - t_start\n",
    "print(f\"Training completed in {t_duration:.2f} seconds.\")\n",
    "\n",
    "# Reconstruct and impute missing values\n",
    "print(\"Reconstructing and imputing missing values...\")\n",
    "X_train_pred = model.predict(X_train_num)\n",
    "# Inverse scaling\n",
    "X_pred_unscaled = num_pipeline.named_steps['scaler'].inverse_transform(X_train_pred)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    mask = df[col].isna().values\n",
    "    if mask.any():\n",
    "        df_imputed.loc[mask, col] = X_pred_unscaled[mask, i]\n",
    "# Generate submission based on sample_submission\n",
    "print(\"Generating submission file...\")\n",
    "predictions = []\n",
    "for rc in df_test[id_col]:\n",
    "    row_str, col_name = rc.split('-')\n",
    "    row_idx = int(row_str)\n",
    "    val = df_imputed.at[row_idx, col_name]\n",
    "    predictions.append(val)\n",
    "\n",
    "submission = pd.DataFrame({id_col: df_test[id_col], target_col: predictions})\n",
    "submission.to_csv('submission_result.csv', index=False)\n",
    "\n",
    "# Save training results\n",
    "print(\"Saving results...\")\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'training_mae': history.history['mae'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'] if 'val_rmse' in history.history else history.history['val_rmse'][-1],\n",
    "    'validation_mae': history.history['val_mae'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.77217316627502\n"
     ]
    }
   ],
   "source": [
    "print(t_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 4 Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing numeric features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     87\u001b[39m tuner = kt.BayesianOptimization(\n\u001b[32m     88\u001b[39m     MyHyperModel(),\n\u001b[32m     89\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     project_name=\u001b[33m'\u001b[39m\u001b[33mbayesian_tuner\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Search for the best hyperparameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my_val\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m     tuner.search(\n\u001b[32m    100\u001b[39m         X_train_num, y_train,\n\u001b[32m    101\u001b[39m         validation_data=(X_val_num, y_val),\n\u001b[32m    102\u001b[39m         batch_size=bs, epochs=ep,\n\u001b[32m    103\u001b[39m         callbacks=[early_stopping, checkpoint]\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf_import_error = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "except ImportError:\n",
    "    tf_import_error = True\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if not tf_import_error:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('tabular-playground-series-jun-2022/data.csv.zip')\n",
    "df_test = pd.read_csv('tabular-playground-series-jun-2022/sample_submission.csv.zip')\n",
    "\n",
    "# Infer identifiers\n",
    "id_col = 'row-col'\n",
    "target_col = 'value'\n",
    "\n",
    "# Identify continuous features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training numeric data\n",
    "print(\"Preprocessing numeric features...\")\n",
    "X_train_num = num_pipeline.fit_transform(df[num_cols])\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_num.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        output_layer = Dense(n_features, activation='linear')(x)\n",
    "        model = Model(inputs, output_layer)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Initialize the tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# Reconstruct and impute missing values\n",
    "print(\"Reconstructing and imputing missing values...\")\n",
    "X_train_pred = model.predict(X_train_num)\n",
    "# Inverse scaling\n",
    "X_pred_unscaled = num_pipeline.named_steps['scaler'].inverse_transform(X_train_pred)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    mask = df[col].isna().values\n",
    "    if mask.any():\n",
    "        df_imputed.loc[mask, col] = X_pred_unscaled[mask, i]\n",
    "# Generate submission based on sample_submission\n",
    "print(\"Generating submission file...\")\n",
    "predictions = []\n",
    "for rc in df_test[id_col]:\n",
    "    row_str, col_name = rc.split('-')\n",
    "    row_idx = int(row_str)\n",
    "    val = df_imputed.at[row_idx, col_name]\n",
    "    predictions.append(val)\n",
    "\n",
    "submission = pd.DataFrame({id_col: df_test[id_col], target_col: predictions})\n",
    "submission.to_csv('submission_result.csv', index=False)\n",
    "\n",
    "# Save training results\n",
    "print(\"Saving results...\")\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'training_mae': history.history['mae'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'] if 'val_rmse' in history.history else history.history['val_rmse'][-1],\n",
    "    'validation_mae': history.history['val_mae'][-1]\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing numeric features...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'value'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocessing numeric features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m X_train_num = num_pipeline.fit_transform(df[num_cols])\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m y_train = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m.values  \u001b[38;5;66;03m# Define y_train\u001b[39;00m\n\u001b[32m     49\u001b[39m X_val_num = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Define X_val_num for validation split\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Keras-Tuner model definition\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'value'"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf_import_error = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "except ImportError:\n",
    "    tf_import_error = True\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if not tf_import_error:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('tabular-playground-series-jun-2022/data.csv.zip')\n",
    "df_test = pd.read_csv('tabular-playground-series-jun-2022/sample_submission.csv.zip')\n",
    "\n",
    "# Infer identifiers\n",
    "id_col = 'row-col'\n",
    "target_col = 'value'\n",
    "\n",
    "# Identify continuous features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training numeric data\n",
    "print(\"Preprocessing numeric features...\")\n",
    "X_train_num = num_pipeline.fit_transform(df[num_cols])\n",
    "y_train = df[target_col].values  # Define y_train\n",
    "X_val_num = None  # Define X_val_num for validation split\n",
    "y_val = None  # Define y_val for validation split\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_num.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        output_layer = Dense(n_features, activation='linear')(x)\n",
    "        model = Model(inputs, output_layer)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Initialize the tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# End timing the training process\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Reconstruct and impute missing values\n",
    "print(\"Reconstructing and imputing missing values...\")\n",
    "X_train_pred = model.predict(X_train_num)\n",
    "# Inverse scaling\n",
    "X_pred_unscaled = num_pipeline.named_steps['scaler'].inverse_transform(X_train_pred)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    mask = df[col].isna().values\n",
    "    if mask.any():\n",
    "        df_imputed.loc[mask, col] = X_pred_unscaled[mask, i]\n",
    "# Generate submission based on sample_submission\n",
    "print(\"Generating submission file...\")\n",
    "predictions = []\n",
    "for rc in df_test[id_col]:\n",
    "    row_str, col_name = rc.split('-')\n",
    "    row_idx = int(row_str)\n",
    "    val = df_imputed.at[row_idx, col_name]\n",
    "    predictions.append(val)\n",
    "\n",
    "submission = pd.DataFrame({id_col: df_test[id_col], target_col: predictions})\n",
    "submission.to_csv('submission_result.csv', index=False)\n",
    "\n",
    "# Save training results\n",
    "print(\"Saving results...\")\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'training_mae': history.history['mae'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1] if 'val_rmse' in history.history else None,\n",
    "    'validation_mae': history.history['val_mae'][-1],\n",
    "    'training_time': training_time  # Save training time\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 52s]\n",
      "val_loss: 3.261597602785028e-14\n",
      "\n",
      "Best val_loss So Far: 3.261597602785028e-14\n",
      "Total elapsed time: 00h 00m 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 50s - 2ms/step - loss: 1.6314e-05 - mae: 3.5707e-04 - rmse: 0.0040 - val_loss: 5.9116e-12 - val_mae: 2.4315e-06 - val_rmse: 2.4314e-06\n",
      "Reconstructing and imputing missing values...\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 639us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1000000,1) doesn't match the broadcast shape (1000000,81)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    144\u001b[39m X_train_pred = model.predict(X_train_num)\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Inverse scaling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m X_pred_unscaled = \u001b[43mnum_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscaler\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m df_imputed = df.copy()\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(num_cols):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1139\u001b[39m, in \u001b[36mStandardScaler.inverse_transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_std:\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m         \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_\u001b[49m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n\u001b[32m   1141\u001b[39m         X += \u001b[38;5;28mself\u001b[39m.mean_\n",
      "\u001b[31mValueError\u001b[39m: non-broadcastable output operand with shape (1000000,1) doesn't match the broadcast shape (1000000,81)"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf_import_error = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "except ImportError:\n",
    "    tf_import_error = True\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if not tf_import_error:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('tabular-playground-series-jun-2022/data.csv.zip')\n",
    "df_test = pd.read_csv('tabular-playground-series-jun-2022/sample_submission.csv.zip')\n",
    "\n",
    "# Infer identifiers\n",
    "id_col = 'row-col'\n",
    "target_col = 'value'\n",
    "\n",
    "# Check if target column exists in the test data\n",
    "if target_col not in df_test.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found in the sample submission dataframe.\")\n",
    "\n",
    "# Identify continuous features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training numeric data\n",
    "print(\"Preprocessing numeric features...\")\n",
    "X_train_num = num_pipeline.fit_transform(df[num_cols])\n",
    "y_train = df_test[target_col].values  # Define y_train from test data\n",
    "X_val_num = None  # Define X_val_num for validation split\n",
    "y_val = None  # Define y_val for validation split\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_num.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        output_layer = Dense(1, activation='linear')(x)  # Change output layer to have 1 unit\n",
    "        model = Model(inputs, output_layer)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Initialize the tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=1,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# End timing the training process\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Reconstruct and impute missing values\n",
    "print(\"Reconstructing and imputing missing values...\")\n",
    "X_train_pred = model.predict(X_train_num)\n",
    "# Inverse scaling\n",
    "X_pred_unscaled = num_pipeline.named_steps['scaler'].inverse_transform(X_train_pred)\n",
    "\n",
    "df_imputed = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    mask = df[col].isna().values\n",
    "    if mask.any():\n",
    "        df_imputed.loc[mask, col] = X_pred_unscaled[mask, i]\n",
    "# Generate submission based on sample_submission\n",
    "print(\"Generating submission file...\")\n",
    "predictions = []\n",
    "for rc in df_test[id_col]:\n",
    "    row_str, col_name = rc.split('-')\n",
    "    row_idx = int(row_str)\n",
    "    val = df_imputed.at[row_idx, col_name]\n",
    "    predictions.append(val)\n",
    "\n",
    "submission = pd.DataFrame({id_col: df_test[id_col], target_col: predictions})\n",
    "submission.to_csv('submission_result.csv', index=False)\n",
    "\n",
    "# Save training results\n",
    "print(\"Saving results...\")\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'training_mae': history.history['mae'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1] if 'val_rmse' in history.history else None,\n",
    "    'validation_mae': history.history['val_mae'][-1],\n",
    "    'training_time': training_time  # Save training time\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 39m 40s]\n",
      "val_loss: 2.292622398850879e-22\n",
      "\n",
      "Best val_loss So Far: 0.0\n",
      "Total elapsed time: 05h 30m 06s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 100s - 4ms/step - loss: 0.0013 - mae: 0.0050 - rmse: 0.0361 - val_loss: 2.3745e-10 - val_mae: 1.5296e-05 - val_rmse: 1.5409e-05\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 104s - 4ms/step - loss: 1.9702e-08 - mae: 1.4401e-05 - rmse: 1.4037e-04 - val_loss: 1.4026e-21 - val_mae: 3.7449e-11 - val_rmse: 3.7451e-11\n",
      "Epoch 3/100\n",
      "25000/25000 - 92s - 4ms/step - loss: 3.1575e-09 - mae: 8.0853e-06 - rmse: 5.6192e-05 - val_loss: 1.7438e-18 - val_mae: 1.3206e-09 - val_rmse: 1.3205e-09\n",
      "Epoch 4/100\n",
      "25000/25000 - 83s - 3ms/step - loss: 1.0912e-09 - mae: 7.7536e-06 - rmse: 3.3033e-05 - val_loss: 2.3983e-12 - val_mae: 1.5488e-06 - val_rmse: 1.5486e-06\n",
      "Epoch 5/100\n",
      "25000/25000 - 82s - 3ms/step - loss: 1.8032e-09 - mae: 7.7024e-06 - rmse: 4.2464e-05 - val_loss: 2.2981e-09 - val_mae: 4.7935e-05 - val_rmse: 4.7938e-05\n",
      "Epoch 6/100\n",
      "25000/25000 - 82s - 3ms/step - loss: 1.1831e-09 - mae: 7.1936e-06 - rmse: 3.4396e-05 - val_loss: 1.7892e-20 - val_mae: 1.3375e-10 - val_rmse: 1.3376e-10\n",
      "Epoch 7/100\n",
      "25000/25000 - 84s - 3ms/step - loss: 2.6238e-09 - mae: 8.4071e-06 - rmse: 5.1224e-05 - val_loss: 1.0325e-12 - val_mae: 1.0161e-06 - val_rmse: 1.0161e-06\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 99s - 4ms/step - loss: 4.7698e-09 - mae: 6.9596e-06 - rmse: 6.9064e-05 - val_loss: 3.1318e-28 - val_mae: 1.7696e-14 - val_rmse: 1.7697e-14\n",
      "Epoch 9/100\n",
      "25000/25000 - 87s - 3ms/step - loss: 9.8873e-10 - mae: 7.1351e-06 - rmse: 3.1444e-05 - val_loss: 8.8613e-19 - val_mae: 9.4133e-10 - val_rmse: 9.4134e-10\n",
      "Epoch 10/100\n",
      "25000/25000 - 92s - 4ms/step - loss: 1.6177e-09 - mae: 7.1248e-06 - rmse: 4.0220e-05 - val_loss: 1.3361e-24 - val_mae: 1.1559e-12 - val_rmse: 1.1559e-12\n",
      "Epoch 11/100\n",
      "25000/25000 - 96s - 4ms/step - loss: 1.0952e-09 - mae: 7.3512e-06 - rmse: 3.3094e-05 - val_loss: 3.2377e-22 - val_mae: 1.7993e-11 - val_rmse: 1.7994e-11\n",
      "Epoch 12/100\n",
      "25000/25000 - 90s - 4ms/step - loss: 1.0238e-09 - mae: 7.2997e-06 - rmse: 3.1997e-05 - val_loss: 3.3752e-25 - val_mae: 5.8092e-13 - val_rmse: 5.8097e-13\n",
      "Epoch 13/100\n",
      "25000/25000 - 95s - 4ms/step - loss: 8.3824e-10 - mae: 7.4283e-06 - rmse: 2.8952e-05 - val_loss: 9.9247e-23 - val_mae: 9.9619e-12 - val_rmse: 9.9623e-12\n",
      "Epoch 14/100\n",
      "25000/25000 - 93s - 4ms/step - loss: 9.5654e-10 - mae: 7.6780e-06 - rmse: 3.0928e-05 - val_loss: 5.6103e-14 - val_mae: 2.3688e-07 - val_rmse: 2.3686e-07\n",
      "Epoch 15/100\n",
      "25000/25000 - 99s - 4ms/step - loss: 8.0071e-10 - mae: 7.5748e-06 - rmse: 2.8297e-05 - val_loss: 1.4368e-19 - val_mae: 3.7903e-10 - val_rmse: 3.7905e-10\n",
      "Epoch 16/100\n",
      "25000/25000 - 102s - 4ms/step - loss: 1.0391e-09 - mae: 7.4260e-06 - rmse: 3.2235e-05 - val_loss: 1.4189e-17 - val_mae: 3.7670e-09 - val_rmse: 3.7669e-09\n",
      "Epoch 17/100\n",
      "25000/25000 - 100s - 4ms/step - loss: 1.7870e-09 - mae: 7.8281e-06 - rmse: 4.2273e-05 - val_loss: 4.0300e-14 - val_mae: 2.0074e-07 - val_rmse: 2.0075e-07\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 96s - 4ms/step - loss: 8.1059e-10 - mae: 7.5536e-06 - rmse: 2.8471e-05 - val_loss: 6.6034e-30 - val_mae: 2.5697e-15 - val_rmse: 2.5697e-15\n",
      "Epoch 19/100\n",
      "25000/25000 - 99s - 4ms/step - loss: 8.0869e-10 - mae: 7.5721e-06 - rmse: 2.8437e-05 - val_loss: 8.0671e-25 - val_mae: 8.9825e-13 - val_rmse: 8.9817e-13\n",
      "Epoch 20/100\n",
      "25000/25000 - 99s - 4ms/step - loss: 8.3784e-10 - mae: 7.5306e-06 - rmse: 2.8945e-05 - val_loss: 8.3632e-14 - val_mae: 2.8918e-07 - val_rmse: 2.8919e-07\n",
      "Epoch 21/100\n",
      "25000/25000 - 98s - 4ms/step - loss: 7.9404e-10 - mae: 7.5836e-06 - rmse: 2.8179e-05 - val_loss: 2.2045e-24 - val_mae: 1.4848e-12 - val_rmse: 1.4848e-12\n",
      "Epoch 22/100\n",
      "25000/25000 - 84s - 3ms/step - loss: 8.1956e-10 - mae: 7.5995e-06 - rmse: 2.8628e-05 - val_loss: 1.6140e-24 - val_mae: 1.2704e-12 - val_rmse: 1.2704e-12\n",
      "Epoch 23/100\n",
      "25000/25000 - 91s - 4ms/step - loss: 1.0234e-09 - mae: 7.2188e-06 - rmse: 3.1991e-05 - val_loss: 2.7571e-16 - val_mae: 1.6604e-08 - val_rmse: 1.6604e-08\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 93s - 4ms/step - loss: 1.3039e-09 - mae: 7.1929e-06 - rmse: 3.6110e-05 - val_loss: 0.0000e+00 - val_mae: 0.0000e+00 - val_rmse: 0.0000e+00\n",
      "Epoch 25/100\n",
      "25000/25000 - 99s - 4ms/step - loss: 9.0556e-10 - mae: 6.9018e-06 - rmse: 3.0093e-05 - val_loss: 2.3404e-20 - val_mae: 1.5298e-10 - val_rmse: 1.5298e-10\n",
      "Epoch 26/100\n",
      "25000/25000 - 99s - 4ms/step - loss: 8.2250e-10 - mae: 7.7547e-06 - rmse: 2.8679e-05 - val_loss: 3.0559e-09 - val_mae: 5.5283e-05 - val_rmse: 5.5280e-05\n",
      "Epoch 27/100\n",
      "25000/25000 - 92s - 4ms/step - loss: 1.4010e-09 - mae: 7.0487e-06 - rmse: 3.7430e-05 - val_loss: 0.0000e+00 - val_mae: 1.5604e-20 - val_rmse: 0.0000e+00\n",
      "Epoch 28/100\n",
      "25000/25000 - 97s - 4ms/step - loss: 8.0021e-10 - mae: 7.6853e-06 - rmse: 2.8288e-05 - val_loss: 1.6498e-23 - val_mae: 4.0620e-12 - val_rmse: 4.0618e-12\n",
      "Epoch 29/100\n",
      "25000/25000 - 102s - 4ms/step - loss: 1.0634e-09 - mae: 7.2130e-06 - rmse: 3.2609e-05 - val_loss: 1.2435e-18 - val_mae: 1.1151e-09 - val_rmse: 1.1151e-09\n",
      "Epoch 30/100\n",
      "25000/25000 - 88s - 4ms/step - loss: 8.0587e-10 - mae: 7.8240e-06 - rmse: 2.8388e-05 - val_loss: 3.7047e-10 - val_mae: 1.9247e-05 - val_rmse: 1.9248e-05\n",
      "Epoch 31/100\n",
      "25000/25000 - 95s - 4ms/step - loss: 8.0441e-10 - mae: 7.5182e-06 - rmse: 2.8362e-05 - val_loss: 5.7210e-17 - val_mae: 7.5634e-09 - val_rmse: 7.5637e-09\n",
      "Epoch 32/100\n",
      "25000/25000 - 96s - 4ms/step - loss: 9.4474e-10 - mae: 7.1419e-06 - rmse: 3.0737e-05 - val_loss: 2.0414e-18 - val_mae: 1.4288e-09 - val_rmse: 1.4288e-09\n",
      "Epoch 33/100\n",
      "25000/25000 - 95s - 4ms/step - loss: 8.2334e-10 - mae: 7.7735e-06 - rmse: 2.8694e-05 - val_loss: 9.2149e-11 - val_mae: 9.6001e-06 - val_rmse: 9.5994e-06\n",
      "Epoch 34/100\n",
      "25000/25000 - 100s - 4ms/step - loss: 8.0333e-10 - mae: 7.5161e-06 - rmse: 2.8343e-05 - val_loss: 1.9794e-16 - val_mae: 1.4069e-08 - val_rmse: 1.4069e-08\n",
      "Reconstructing and imputing missing values...\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1ms/step\n",
      "Generating submission file...\n",
      "Saving results...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tf_import_error = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "except ImportError:\n",
    "    tf_import_error = True\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if not tf_import_error:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('data.csv.zip')\n",
    "df_test = pd.read_csv('sample_submission.csv.zip')\n",
    "\n",
    "# Infer identifiers\n",
    "id_col = 'row-col'\n",
    "target_col = 'value'\n",
    "\n",
    "# Check if target column exists in the test data\n",
    "if target_col not in df_test.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found in the sample submission dataframe.\")\n",
    "\n",
    "# Identify continuous features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "\n",
    "# Preprocessing pipeline for numeric features\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Fit and transform training numeric data\n",
    "print(\"Preprocessing numeric features...\")\n",
    "X_train_num = num_pipeline.fit_transform(df[num_cols])\n",
    "y_train = df_test[target_col].values  # Define y_train from test data\n",
    "X_val_num = None  # Define X_val_num for validation split\n",
    "y_val = None  # Define y_val for validation split\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_num.shape[1]\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        output_layer = Dense(1, activation='linear')(x)  # Change output layer to have 1 unit\n",
    "        model = Model(inputs, output_layer)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
    "        return model\n",
    "\n",
    "# Initialize the tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_data=(X_val_num, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_num, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# End timing the training process\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Reconstruct and impute missing values\n",
    "print(\"Reconstructing and imputing missing values...\")\n",
    "X_train_pred = model.predict(X_train_num)\n",
    "# Inverse scaling\n",
    "X_pred_unscaled = num_pipeline.named_steps['scaler'].inverse_transform(np.repeat(X_train_pred, n_features, axis=1))\n",
    "\n",
    "df_imputed = df.copy()\n",
    "for i, col in enumerate(num_cols):\n",
    "    mask = df[col].isna().values\n",
    "    if mask.any():\n",
    "        df_imputed.loc[mask, col] = X_pred_unscaled[mask, i]\n",
    "# Generate submission based on sample_submission\n",
    "print(\"Generating submission file...\")\n",
    "predictions = []\n",
    "for rc in df_test[id_col]:\n",
    "    row_str, col_name = rc.split('-')\n",
    "    row_idx = int(row_str)\n",
    "    val = df_imputed.at[row_idx, col_name]\n",
    "    predictions.append(val)\n",
    "\n",
    "submission = pd.DataFrame({id_col: df_test[id_col], target_col: predictions})\n",
    "submission.to_csv('submission_result.csv', index=False)\n",
    "\n",
    "# Save training results\n",
    "print(\"Saving results...\")\n",
    "results = {\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'training_rmse': history.history['rmse'][-1],\n",
    "    'training_mae': history.history['mae'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'validation_rmse': history.history['val_rmse'][-1] if 'val_rmse' in history.history else None,\n",
    "    'validation_mae': history.history['val_mae'][-1],\n",
    "    'training_time': training_time  # Save training time\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Time saved in results.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
