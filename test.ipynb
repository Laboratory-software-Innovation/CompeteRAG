{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 12s - 1ms/step - loss: 1.1184 - mae: 0.7020 - rmse: 1.0576 - val_loss: 0.1161 - val_mae: 0.2319 - val_rmse: 0.3408\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9375/9375 - 11s - 1ms/step - loss: 0.1690 - mae: 0.2813 - rmse: 0.4111 - val_loss: 0.0895 - val_mae: 0.2125 - val_rmse: 0.2991\n",
      "Epoch 3/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1613 - mae: 0.2749 - rmse: 0.4016 - val_loss: 0.0996 - val_mae: 0.2212 - val_rmse: 0.3156\n",
      "Epoch 4/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1597 - mae: 0.2729 - rmse: 0.3996 - val_loss: 0.1386 - val_mae: 0.2598 - val_rmse: 0.3723\n",
      "Epoch 5/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1590 - mae: 0.2722 - rmse: 0.3987 - val_loss: 0.2138 - val_mae: 0.3036 - val_rmse: 0.4624\n",
      "Epoch 6/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1509 - mae: 0.2642 - rmse: 0.3884 - val_loss: 0.1937 - val_mae: 0.2894 - val_rmse: 0.4402\n",
      "Epoch 7/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1516 - mae: 0.2643 - rmse: 0.3894 - val_loss: 0.1977 - val_mae: 0.3071 - val_rmse: 0.4446\n",
      "Epoch 8/100\n",
      "9375/9375 - 20s - 2ms/step - loss: 0.1519 - mae: 0.2642 - rmse: 0.3897 - val_loss: 0.1819 - val_mae: 0.2799 - val_rmse: 0.4266\n",
      "Epoch 9/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1520 - mae: 0.2644 - rmse: 0.3899 - val_loss: 0.1888 - val_mae: 0.2796 - val_rmse: 0.4346\n",
      "Epoch 10/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1512 - mae: 0.2637 - rmse: 0.3889 - val_loss: 0.1987 - val_mae: 0.2908 - val_rmse: 0.4457\n",
      "Epoch 11/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1506 - mae: 0.2636 - rmse: 0.3881 - val_loss: 0.1952 - val_mae: 0.2894 - val_rmse: 0.4418\n",
      "Epoch 12/100\n",
      "9375/9375 - 11s - 1ms/step - loss: 0.1520 - mae: 0.2644 - rmse: 0.3898 - val_loss: 0.1868 - val_mae: 0.2907 - val_rmse: 0.4322\n",
      "\u001b[1m7813/7813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 534us/step\n"
     ]
    }
   ],
   "source": [
    "import random, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "sample_sub = pd.read_csv('playground-series-s5e5/sample_submission.csv')\n",
    "id_col = sample_sub.columns[0]\n",
    "target_columns = list(sample_sub.columns[1:])\n",
    "\n",
    "df_train = pd.read_csv('playground-series-s5e5/train.csv')\n",
    "df_test = pd.read_csv('playground-series-s5e5/test.csv')\n",
    "\n",
    "df = df_train.copy()\n",
    "\n",
    "# Target encoding for regression\n",
    "y_values = df[target_columns].astype(float).values\n",
    "# apply log1p since values >= 0\n",
    "y_enc = np.log1p(y_values)\n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "\n",
    "# Split (use provided test)\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "train_ids = df[id_col]\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering: drop all-missing\n",
    "all_missing = [c for c in X_train.columns if X_train[c].isna().all()]\n",
    "X_train.drop(columns=all_missing, inplace=True)\n",
    "X_val.drop(columns=all_missing, inplace=True, errors='ignore')\n",
    "# Categorical handling\n",
    "categorical = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "high_card = [c for c in categorical if X_train[c].nunique() > 50]\n",
    "X_train.drop(columns=high_card, inplace=True)\n",
    "X_val.drop(columns=high_card, inplace=True, errors='ignore')\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# Model architecture guidelines for small dataset\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "units1 = min(n_features * 2, 128)\n",
    "units2 = min(n_features, 64)\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "x = Dense(units1, activation='relu')(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(units2, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(len(target_columns), activation='linear')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[RootMeanSquaredError(name='rmse'), MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# Callbacks & Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('model_best.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "start_time = time.time()\n",
    "if y_val is not None:\n",
    "    history = model.fit(X_train_proc, y_train, validation_data=(X_val_proc, y_val),\n",
    "                        epochs=100, batch_size=64, callbacks=callbacks, verbose=2)\n",
    "else:\n",
    "    history = model.fit(X_train_proc, y_train, validation_split=0.2,\n",
    "                        epochs=100, batch_size=64, callbacks=callbacks, verbose=2)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Evaluation & Logging\n",
    "hist = history.history\n",
    "results = {\n",
    "    'training_accuracy': float(hist['rmse'][-1]),\n",
    "    'training_loss': float(hist['loss'][-1]),\n",
    "    'validation_accuracy': float(hist['val_rmse'][-1]),\n",
    "    'validation_loss': float(hist['val_loss'][-1])\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "# inverse log1p\n",
    "final = np.expm1(raw_preds)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
