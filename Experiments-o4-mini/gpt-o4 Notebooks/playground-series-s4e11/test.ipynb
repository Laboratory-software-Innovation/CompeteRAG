{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Mental Health Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:28:57.975409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752150537.992814 1689569 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752150537.997935 1689569 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752150538.012742 1689569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752150538.012755 1689569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752150538.012757 1689569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752150538.012759 1689569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 12:28:58.019111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1752150540.911841 1689569 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 875 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:04:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752150542.736409 1689710 service.cc:152] XLA service 0x786b58009d30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752150542.736437 1689710 service.cc:160]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-10 12:29:02.801371: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1752150542.996289 1689710 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "I0000 00:00:1752150544.376291 1689710 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-07-10 12:29:09.757329: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:09.855315: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:09.942353: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:12.148666: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:12.170379: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:12.239288: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-07-10 12:29:12.283606: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_50', 248 bytes spill stores, 252 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.15206, saving model to model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 - 11s - 13ms/step - accuracy: 0.9286 - auc: 0.9644 - loss: 0.1779 - val_accuracy: 0.9399 - val_auc: 0.9749 - val_loss: 0.1521\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.15206 to 0.15199, saving model to model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 - 3s - 4ms/step - accuracy: 0.9364 - auc: 0.9705 - loss: 0.1610 - val_accuracy: 0.9398 - val_auc: 0.9750 - val_loss: 0.1520\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.15199 to 0.15121, saving model to model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/880 - 3s - 4ms/step - accuracy: 0.9369 - auc: 0.9710 - loss: 0.1598 - val_accuracy: 0.9399 - val_auc: 0.9752 - val_loss: 0.1512\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9373 - auc: 0.9716 - loss: 0.1582 - val_accuracy: 0.9401 - val_auc: 0.9751 - val_loss: 0.1514\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9377 - auc: 0.9716 - loss: 0.1580 - val_accuracy: 0.9395 - val_auc: 0.9751 - val_loss: 0.1516\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9374 - auc: 0.9718 - loss: 0.1575 - val_accuracy: 0.9399 - val_auc: 0.9751 - val_loss: 0.1517\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9377 - auc: 0.9720 - loss: 0.1573 - val_accuracy: 0.9397 - val_auc: 0.9752 - val_loss: 0.1517\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9375 - auc: 0.9719 - loss: 0.1575 - val_accuracy: 0.9397 - val_auc: 0.9752 - val_loss: 0.1516\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9376 - auc: 0.9723 - loss: 0.1568 - val_accuracy: 0.9400 - val_auc: 0.9750 - val_loss: 0.1518\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9382 - auc: 0.9721 - loss: 0.1566 - val_accuracy: 0.9398 - val_auc: 0.9750 - val_loss: 0.1517\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9378 - auc: 0.9723 - loss: 0.1560 - val_accuracy: 0.9399 - val_auc: 0.9749 - val_loss: 0.1517\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 4ms/step - accuracy: 0.9381 - auc: 0.9722 - loss: 0.1567 - val_accuracy: 0.9402 - val_auc: 0.9748 - val_loss: 0.1520\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.15121\n",
      "880/880 - 3s - 3ms/step - accuracy: 0.9381 - auc: 0.9721 - loss: 0.1564 - val_accuracy: 0.9394 - val_auc: 0.9748 - val_loss: 0.1520\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:29:51.889754: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2932/2932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data Loading\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Infer id and target columns\n",
    "id_col = 'id'\n",
    "target_col = 'Depression'\n",
    "# Map train target if necessary\n",
    "if target_col not in train_df.columns and 'class' in train_df.columns:\n",
    "    train_df[target_col] = train_df['class']\n",
    "\n",
    "# Encode target\n",
    "ename = LabelEncoder()\n",
    "y_enc = ename.fit_transform(train_df[target_col].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = train_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "train_ids = train_df[id_col]\n",
    "\n",
    "X_test = test_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "test_ids = test_df[id_col]\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering\n",
    "# Drop all-missing columns\n",
    "cols_all_missing = [col for col in X_train.columns if X_train[col].isna().all()]\n",
    "X_train = X_train.drop(columns=cols_all_missing)\n",
    "X_test = X_test.drop(columns=cols_all_missing)\n",
    "\n",
    "# Identify categorical columns with cardinality <=50\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cols = [col for col in cat_cols if X_train[col].nunique() <= 50]\n",
    "# Numeric columns\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, low_card_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model architecture\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "# Two hidden layers for small dataset\n",
    "layer1_units = min(n_features * 2, 128)\n",
    "layer2_units = min(n_features, 64)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(n_features,))\n",
    "x = tf.keras.layers.Dense(layer1_units, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(layer2_units, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', AUC(name='auc')])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('model_checkpoint.h5', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_accuracy': history.history['accuracy'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_accuracy'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'training_duration_seconds': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction and submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final_preds = (probs > 0.5).astype(int)\n",
    "\n",
    "# Build submission\n",
    "submission = pd.DataFrame({id_col: test_ids.reset_index(drop=True), target_col: final_preds})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.26095795631409\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 2 Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 11s]\n",
      "val_loss: 0.15245185792446136\n",
      "\n",
      "Best val_loss So Far: 0.15245185792446136\n",
      "Total elapsed time: 00h 00m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 - 10s - 3ms/step - accuracy: 0.9324 - auc: 0.9662 - loss: 0.1724 - val_accuracy: 0.9386 - val_auc: 0.9741 - val_loss: 0.1531\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'duration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    149\u001b[39m     history = model.fit(\n\u001b[32m    150\u001b[39m         X_train_proc, y_enc,\n\u001b[32m    151\u001b[39m         validation_split=\u001b[32m0.2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m         verbose=\u001b[32m2\u001b[39m\n\u001b[32m    155\u001b[39m     )\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Logging results\u001b[39;00m\n\u001b[32m    158\u001b[39m results = {\n\u001b[32m    159\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_accuracy\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    160\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_loss\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    161\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidation_accuracy\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m    162\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidation_loss\u001b[39m\u001b[33m'\u001b[39m: history.history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_duration_seconds\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mduration\u001b[49m\n\u001b[32m    164\u001b[39m }\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mresults.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    166\u001b[39m     json.dump(results, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'duration' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import json\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data Loading\n",
    "train_df = pd.read_csv('playground-series-s4e11/train.csv')\n",
    "test_df = pd.read_csv('playground-series-s4e11/test.csv')\n",
    "\n",
    "# Infer id and target columns\n",
    "id_col = 'id'\n",
    "target_col = 'Depression'\n",
    "# Map train target if necessary\n",
    "if target_col not in train_df.columns and 'class' in train_df.columns:\n",
    "    train_df[target_col] = train_df['class']\n",
    "\n",
    "# Encode target\n",
    "ename = LabelEncoder()\n",
    "y_enc = ename.fit_transform(train_df[target_col].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = train_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "train_ids = train_df[id_col]\n",
    "\n",
    "X_test = test_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "test_ids = test_df[id_col]\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering\n",
    "# Drop all-missing columns\n",
    "cols_all_missing = [col for col in X_train.columns if X_train[col].isna().all()]\n",
    "X_train = X_train.drop(columns=cols_all_missing)\n",
    "X_test = X_test.drop(columns=cols_all_missing)\n",
    "\n",
    "# Identify categorical columns with cardinality <=50\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cols = [col for col in cat_cols if X_train[col].nunique() <= 50]\n",
    "# Numeric columns\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, low_card_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model architecture\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "# HyperModel class\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "# Build the best model\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain the model with original callbacks and data\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_accuracy': history.history['accuracy'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_accuracy'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'training_duration_seconds': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction and submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final_preds = (probs > 0.5).astype(int)\n",
    "\n",
    "# Build submission\n",
    "submission = pd.DataFrame({id_col: test_ids.reset_index(drop=True), target_col: final_preds})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 04m 00s]\n",
      "val_loss: 0.1520763337612152\n",
      "\n",
      "Best val_loss So Far: 0.15107053518295288\n",
      "Total elapsed time: 00h 44m 06s\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 - 18s - 5ms/step - accuracy: 0.9317 - auc: 0.9677 - loss: 0.1690 - val_accuracy: 0.9397 - val_auc: 0.9749 - val_loss: 0.1516\n",
      "Epoch 2/20\n",
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9364 - auc: 0.9703 - loss: 0.1612 - val_accuracy: 0.9391 - val_auc: 0.9750 - val_loss: 0.1517\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9369 - auc: 0.9707 - loss: 0.1602 - val_accuracy: 0.9400 - val_auc: 0.9750 - val_loss: 0.1513\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9367 - auc: 0.9709 - loss: 0.1593 - val_accuracy: 0.9402 - val_auc: 0.9751 - val_loss: 0.1512\n",
      "Epoch 5/20\n",
      "3518/3518 - 14s - 4ms/step - accuracy: 0.9368 - auc: 0.9712 - loss: 0.1590 - val_accuracy: 0.9399 - val_auc: 0.9751 - val_loss: 0.1515\n",
      "Epoch 6/20\n",
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9374 - auc: 0.9714 - loss: 0.1589 - val_accuracy: 0.9399 - val_auc: 0.9750 - val_loss: 0.1518\n",
      "Epoch 7/20\n",
      "3518/3518 - 12s - 4ms/step - accuracy: 0.9371 - auc: 0.9711 - loss: 0.1590 - val_accuracy: 0.9394 - val_auc: 0.9748 - val_loss: 0.1522\n",
      "Epoch 8/20\n",
      "3518/3518 - 12s - 3ms/step - accuracy: 0.9376 - auc: 0.9715 - loss: 0.1583 - val_accuracy: 0.9394 - val_auc: 0.9748 - val_loss: 0.1518\n",
      "Epoch 9/20\n",
      "3518/3518 - 12s - 4ms/step - accuracy: 0.9373 - auc: 0.9717 - loss: 0.1582 - val_accuracy: 0.9397 - val_auc: 0.9749 - val_loss: 0.1520\n",
      "Epoch 10/20\n",
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9379 - auc: 0.9715 - loss: 0.1582 - val_accuracy: 0.9398 - val_auc: 0.9749 - val_loss: 0.1523\n",
      "Epoch 11/20\n",
      "3518/3518 - 14s - 4ms/step - accuracy: 0.9379 - auc: 0.9718 - loss: 0.1577 - val_accuracy: 0.9393 - val_auc: 0.9748 - val_loss: 0.1519\n",
      "Epoch 12/20\n",
      "3518/3518 - 15s - 4ms/step - accuracy: 0.9375 - auc: 0.9718 - loss: 0.1582 - val_accuracy: 0.9395 - val_auc: 0.9748 - val_loss: 0.1519\n",
      "Epoch 13/20\n",
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9379 - auc: 0.9715 - loss: 0.1579 - val_accuracy: 0.9399 - val_auc: 0.9749 - val_loss: 0.1519\n",
      "Epoch 14/20\n",
      "3518/3518 - 13s - 4ms/step - accuracy: 0.9375 - auc: 0.9715 - loss: 0.1581 - val_accuracy: 0.9397 - val_auc: 0.9749 - val_loss: 0.1518\n",
      "\u001b[1m2922/2932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 22:46:49.568984: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-07-15 22:46:49.587631: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 48 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-15 22:46:49.646527: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 48 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-15 22:46:49.878442: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 748 bytes spill stores, 748 bytes spill loads\n",
      "\n",
      "2025-07-15 22:46:50.000843: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_16', 696 bytes spill stores, 696 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2932/2932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import json\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data Loading\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Infer id and target columns\n",
    "id_col = 'id'\n",
    "target_col = 'Depression'\n",
    "# Map train target if necessary\n",
    "if target_col not in train_df.columns and 'class' in train_df.columns:\n",
    "    train_df[target_col] = train_df['class']\n",
    "\n",
    "# Encode target\n",
    "ename = LabelEncoder()\n",
    "y_enc = ename.fit_transform(train_df[target_col].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = train_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "train_ids = train_df[id_col]\n",
    "\n",
    "X_test = test_df.drop(columns=[target_col, id_col], errors='ignore')\n",
    "test_ids = test_df[id_col]\n",
    "y_val = None\n",
    "\n",
    "# Feature engineering\n",
    "# Drop all-missing columns\n",
    "cols_all_missing = [col for col in X_train.columns if X_train[col].isna().all()]\n",
    "X_train = X_train.drop(columns=cols_all_missing)\n",
    "X_test = X_test.drop(columns=cols_all_missing)\n",
    "\n",
    "# Identify categorical columns with cardinality <=50\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cols = [col for col in cat_cols if X_train[col].nunique() <= 50]\n",
    "# Numeric columns\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, low_card_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model architecture\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Input dimension\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "# HyperModel class\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # batch size\n",
    "ep = 20  # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "# Build the best model\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain the model with original callbacks and data\n",
    "start_time = time.time()  # Start timing\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=ep, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_enc,\n",
    "        validation_split=0.2,\n",
    "        epochs=ep, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "end_time = time.time()  # End timing\n",
    "duration = end_time - start_time  # Calculate duration\n",
    "\n",
    "# Logging results\n",
    "results = {\n",
    "    'training_accuracy': history.history['accuracy'][-1],\n",
    "    'training_loss': history.history['loss'][-1],\n",
    "    'validation_accuracy': history.history['val_accuracy'][-1],\n",
    "    'validation_loss': history.history['val_loss'][-1],\n",
    "    'training_duration_seconds': duration\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction and submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final_preds = (probs > 0.5).astype(int)\n",
    "\n",
    "# Build submission\n",
    "submission = pd.DataFrame({id_col: test_ids.reset_index(drop=True), target_col: final_preds})\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.128700017929\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
