{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding Challenge II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 22:45:58.620654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752101158.637458  930396 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752101158.642520  930396 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752101158.656926  930396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101158.656939  930396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101158.656941  930396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752101158.656943  930396 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 22:45:58.662093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-09 22:46:08.933740: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 - 8s - 2ms/step - accuracy: 0.8144 - auc: 0.7179 - loss: 0.4359 - val_accuracy: 0.8165 - val_auc: 0.7345 - val_loss: 0.4265\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 - 7s - 2ms/step - accuracy: 0.8157 - auc: 0.7302 - loss: 0.4299 - val_accuracy: 0.8166 - val_auc: 0.7357 - val_loss: 0.4258\n",
      "Epoch 3/100\n",
      "3750/3750 - 7s - 2ms/step - accuracy: 0.8157 - auc: 0.7328 - loss: 0.4287 - val_accuracy: 0.8166 - val_auc: 0.7355 - val_loss: 0.4260\n",
      "Epoch 4/100\n",
      "3750/3750 - 7s - 2ms/step - accuracy: 0.8158 - auc: 0.7339 - loss: 0.4282 - val_accuracy: 0.8167 - val_auc: 0.7356 - val_loss: 0.4259\n",
      "Epoch 5/100\n",
      "3750/3750 - 7s - 2ms/step - accuracy: 0.8162 - auc: 0.7352 - loss: 0.4275 - val_accuracy: 0.8169 - val_auc: 0.7355 - val_loss: 0.4263\n",
      "Epoch 6/100\n",
      "3750/3750 - 6s - 2ms/step - accuracy: 0.8157 - auc: 0.7360 - loss: 0.4272 - val_accuracy: 0.8165 - val_auc: 0.7354 - val_loss: 0.4262\n",
      "Epoch 7/100\n",
      "3750/3750 - 6s - 2ms/step - accuracy: 0.8160 - auc: 0.7366 - loss: 0.4269 - val_accuracy: 0.8166 - val_auc: 0.7353 - val_loss: 0.4261\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 575us/step\n"
     ]
    }
   ],
   "source": [
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train_files = ['cat-in-the-dat-ii/train.csv.zip']\n",
    "df_list = [pd.read_csv(f) for f in train_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_test = pd.read_csv('cat-in-the-dat-ii/test.csv.zip')\n",
    "\n",
    "# Infer ids and targets\n",
    "id_col = 'id'\n",
    "target_columns = ['target']\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder().fit(df[target_columns[0]].astype(str))\n",
    "y_train = le.transform(df[target_columns[0]].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing columns\n",
    "missing_cols = X_train.columns[X_train.isna().all()].tolist()\n",
    "X_train = X_train.drop(columns=missing_cols)\n",
    "X_test = X_test.drop(columns=missing_cols)\n",
    "\n",
    "# Identify numeric and categorical\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove high-cardinality categoricals\n",
    "low_cardinality = [c for c in categorical_features if X_train[c].nunique() <= 50]\n",
    "high_card_cols = [c for c in categorical_features if c not in low_cardinality]\n",
    "X_train = X_train.drop(columns=high_card_cols)\n",
    "X_test = X_test.drop(columns=high_card_cols)\n",
    "categorical_features = low_cardinality\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model building\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "inputs = Input(shape=(n_features,))\n",
    "\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units = [min(n_features*2, 128), min(n_features, 64)]\n",
    "    x = inputs\n",
    "    for u in units:\n",
    "        x = Dense(int(u), activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "else:\n",
    "    sizes = [min(n_features*i, 1024) for i in (2, 1, 0.5, 0.25)]\n",
    "    sizes = [int(s) for s in sizes if s >= 16]\n",
    "    x = inputs\n",
    "    for u in sizes:\n",
    "        x = Dense(u, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "last = len(history.history['loss']) - 1\n",
    "results = {\n",
    "    'training_accuracy': float(history.history['accuracy'][last]),\n",
    "    'training_loss': float(history.history['loss'][last]),\n",
    "    'validation_accuracy': float(history.history['val_accuracy'][last]),\n",
    "    'validation_loss': float(history.history['val_loss'][last])\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final = probs\n",
    "\n",
    "# Ensure correct shape\n",
    "final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner - 2 Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 23:56:51.704800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752364611.721291 1095551 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752364611.726308 1095551 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752364611.740459 1095551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752364611.740474 1095551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752364611.740477 1095551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752364611.740479 1095551 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-12 23:56:51.745314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-12 23:57:02.674410: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    104\u001b[39m ep = \u001b[32m20\u001b[39m   \u001b[38;5;66;03m# Example epochs\u001b[39;00m\n\u001b[32m    106\u001b[39m tuner = kt.BayesianOptimization(\n\u001b[32m    107\u001b[39m     MyHyperModel(),\n\u001b[32m    108\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    113\u001b[39m     project_name=\u001b[33m'\u001b[39m\u001b[33mbayesian_tuner\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    114\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my_val\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    117\u001b[39m     tuner.search(\n\u001b[32m    118\u001b[39m         X_train_proc, y_train,\n\u001b[32m    119\u001b[39m         validation_data=(X_val_proc, y_val),\n\u001b[32m    120\u001b[39m         batch_size=\u001b[32m1\u001b[39m, epochs=ep,\n\u001b[32m    121\u001b[39m         callbacks=[early_stopping, checkpoint]\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'y_val' is not defined"
     ]
    }
   ],
   "source": [
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train_files = ['cat-in-the-dat-ii/train.csv.zip']\n",
    "df_list = [pd.read_csv(f) for f in train_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_test = pd.read_csv('cat-in-the-dat-ii/test.csv.zip')\n",
    "\n",
    "# Infer ids and targets\n",
    "id_col = 'id'\n",
    "target_columns = ['target']\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder().fit(df[target_columns[0]].astype(str))\n",
    "y_train = le.transform(df[target_columns[0]].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing columns\n",
    "missing_cols = X_train.columns[X_train.isna().all()].tolist()\n",
    "X_train = X_train.drop(columns=missing_cols)\n",
    "X_test = X_test.drop(columns=missing_cols)\n",
    "\n",
    "# Identify numeric and categorical\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove high-cardinality categoricals\n",
    "low_cardinality = [c for c in categorical_features if X_train[c].nunique() <= 50]\n",
    "high_card_cols = [c for c in categorical_features if c not in low_cardinality]\n",
    "X_train = X_train.drop(columns=high_card_cols)\n",
    "X_test = X_test.drop(columns=high_card_cols)\n",
    "categorical_features = low_cardinality\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model building\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "inputs = Input(shape=(n_features,))\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # Example batch size\n",
    "ep = 20   # Example epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=1,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=True,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=1, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=1, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=1, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=1,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "last = len(history.history['loss']) - 1\n",
    "results = {\n",
    "    'training_accuracy': float(history.history['accuracy'][last]),\n",
    "    'training_loss': float(history.history['loss'][last]),\n",
    "    'validation_accuracy': float(history.history['val_accuracy'][last]),\n",
    "    'validation_loss': float(history.history['val_loss'][last])\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final = probs\n",
    "\n",
    "# Ensure correct shape\n",
    "final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./bayesian_tuner/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752762763.074851 3942539 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37024 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:04:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752762767.030410 3944212 service.cc:152] XLA service 0x723c00003a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752762767.030513 3944212 service.cc:160]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-17 14:32:47.133724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1752762767.523375 3944212 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-07-17 14:32:49.316536: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-17 14:32:49.746624: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_687', 12 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1752762770.840485 3944212 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 - 80s - 5ms/step - accuracy: 0.8146 - auc: 0.7270 - loss: 0.4316 - val_accuracy: 0.8166 - val_auc: 0.7341 - val_loss: 0.4265\n",
      "Epoch 2/100\n",
      "15000/15000 - 58s - 4ms/step - accuracy: 0.8160 - auc: 0.7338 - loss: 0.4280 - val_accuracy: 0.8166 - val_auc: 0.7351 - val_loss: 0.4270\n",
      "Epoch 3/100\n",
      "15000/15000 - 66s - 4ms/step - accuracy: 0.8164 - auc: 0.7359 - loss: 0.4269 - val_accuracy: 0.8169 - val_auc: 0.7351 - val_loss: 0.4272\n",
      "Epoch 4/100\n",
      "15000/15000 - 68s - 5ms/step - accuracy: 0.8168 - auc: 0.7373 - loss: 0.4260 - val_accuracy: 0.8165 - val_auc: 0.7345 - val_loss: 0.4276\n",
      "Epoch 5/100\n",
      "15000/15000 - 57s - 4ms/step - accuracy: 0.8173 - auc: 0.7388 - loss: 0.4252 - val_accuracy: 0.8166 - val_auc: 0.7343 - val_loss: 0.4282\n",
      "Epoch 6/100\n",
      "15000/15000 - 56s - 4ms/step - accuracy: 0.8176 - auc: 0.7402 - loss: 0.4244 - val_accuracy: 0.8168 - val_auc: 0.7337 - val_loss: 0.4283\n",
      "Epoch 7/100\n",
      "15000/15000 - 54s - 4ms/step - accuracy: 0.8180 - auc: 0.7415 - loss: 0.4236 - val_accuracy: 0.8165 - val_auc: 0.7326 - val_loss: 0.4290\n",
      "Epoch 8/100\n",
      "15000/15000 - 54s - 4ms/step - accuracy: 0.8184 - auc: 0.7430 - loss: 0.4226 - val_accuracy: 0.8158 - val_auc: 0.7313 - val_loss: 0.4295\n",
      "Epoch 9/100\n",
      "15000/15000 - 60s - 4ms/step - accuracy: 0.8189 - auc: 0.7444 - loss: 0.4217 - val_accuracy: 0.8156 - val_auc: 0.7306 - val_loss: 0.4296\n",
      "Epoch 10/100\n",
      "15000/15000 - 59s - 4ms/step - accuracy: 0.8193 - auc: 0.7458 - loss: 0.4209 - val_accuracy: 0.8152 - val_auc: 0.7306 - val_loss: 0.4298\n",
      "Epoch 11/100\n",
      "15000/15000 - 60s - 4ms/step - accuracy: 0.8197 - auc: 0.7468 - loss: 0.4202 - val_accuracy: 0.8146 - val_auc: 0.7300 - val_loss: 0.4306\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:44:01.449618: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:01.580844: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.264960: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.320257: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_738', 152 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.441850: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 576 bytes spill stores, 1424 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.450859: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_738', 296 bytes spill stores, 296 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.597380: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_738', 280 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.602900: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_738', 616 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:02.709410: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_738', 776 bytes spill stores, 1892 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:21.525260: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:21.656683: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-17 14:44:21.784442: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 - 24s - 7ms/step - accuracy: 0.8163 - auc: 0.7365 - loss: 0.4265 - val_accuracy: 0.8168 - val_auc: 0.7357 - val_loss: 0.4257\n",
      "Epoch 2/100\n",
      "3750/3750 - 16s - 4ms/step - accuracy: 0.8167 - auc: 0.7376 - loss: 0.4258 - val_accuracy: 0.8167 - val_auc: 0.7356 - val_loss: 0.4257\n",
      "Epoch 3/100\n",
      "3750/3750 - 16s - 4ms/step - accuracy: 0.8172 - auc: 0.7386 - loss: 0.4252 - val_accuracy: 0.8167 - val_auc: 0.7351 - val_loss: 0.4258\n",
      "Epoch 4/100\n",
      "3750/3750 - 17s - 5ms/step - accuracy: 0.8174 - auc: 0.7399 - loss: 0.4244 - val_accuracy: 0.8164 - val_auc: 0.7342 - val_loss: 0.4264\n",
      "Epoch 5/100\n",
      "3750/3750 - 16s - 4ms/step - accuracy: 0.8178 - auc: 0.7411 - loss: 0.4237 - val_accuracy: 0.8162 - val_auc: 0.7334 - val_loss: 0.4269\n",
      "Epoch 6/100\n",
      "3750/3750 - 17s - 4ms/step - accuracy: 0.8183 - auc: 0.7424 - loss: 0.4228 - val_accuracy: 0.8156 - val_auc: 0.7322 - val_loss: 0.4279\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import os, random, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train_files = ['train.csv.zip']\n",
    "df_list = [pd.read_csv(f) for f in train_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df_test = pd.read_csv('test.csv.zip')\n",
    "\n",
    "# Infer ids and targets\n",
    "id_col = 'id'\n",
    "target_columns = ['target']\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder().fit(df[target_columns[0]].astype(str))\n",
    "y_train = le.transform(df[target_columns[0]].astype(str)).astype(int)\n",
    "\n",
    "# Prepare features\n",
    "X_train = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_test = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "test_ids = df_test[id_col]\n",
    "\n",
    "# Feature engineering: drop all-missing columns\n",
    "missing_cols = X_train.columns[X_train.isna().all()].tolist()\n",
    "X_train = X_train.drop(columns=missing_cols)\n",
    "X_test = X_test.drop(columns=missing_cols)\n",
    "\n",
    "# Identify numeric and categorical\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove high-cardinality categoricals\n",
    "low_cardinality = [c for c in categorical_features if X_train[c].nunique() <= 50]\n",
    "high_card_cols = [c for c in categorical_features if c not in low_cardinality]\n",
    "X_train = X_train.drop(columns=high_card_cols)\n",
    "X_test = X_test.drop(columns=high_card_cols)\n",
    "categorical_features = low_cardinality\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Model building\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "inputs = Input(shape=(n_features,))\n",
    "\n",
    "# Keras-Tuner model definition\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, step=64)\n",
    "        act = hp.Choice('activation', ['relu'])\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation=act)(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "# Tuner setup\n",
    "bs = 32  # Example batch size\n",
    "ep = 20   # Example epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=False,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "# Split data for validation\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_proc, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "tuner.search(\n",
    "    X_train_split, y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    batch_size=bs, epochs=ep,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "model = tuner.hypermodel.build(\n",
    "    tuner.get_best_hyperparameters(1)[0]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Retrain model with original callbacks and data\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100, batch_size=bs,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# Logging results\n",
    "last = len(history.history['loss']) - 1\n",
    "results = {\n",
    "    'training_accuracy': float(history.history['accuracy'][last]),\n",
    "    'training_loss': float(history.history['loss'][last]),\n",
    "    'validation_accuracy': float(history.history['val_accuracy'][last]),\n",
    "    'validation_loss': float(history.history['val_loss'][last])\n",
    "}\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Prediction & Submission\n",
    "raw_preds = model.predict(X_test_proc)\n",
    "probs = raw_preds.flatten()\n",
    "final = probs\n",
    "\n",
    "# Ensure correct shape\n",
    "final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778.6314671039581\n"
     ]
    }
   ],
   "source": [
    "#For some reason there are two model.fit blocks, but it doesn't affect the overall score.\n",
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
