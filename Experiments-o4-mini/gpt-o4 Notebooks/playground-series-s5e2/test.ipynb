{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpack Prediction Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified the metric from `log1p RMSE` to Raw `RMSE` for both Keras and Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - 1 Attempt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
        "name": "stderr",
        "output_type": "stream",
        "text": [
        "2025-07-09 21:25:04.887946: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
        ]
    },
    {
    "name": "stdout",
    "output_type": "stream",
    "text": [
        "Epoch 1/100\n",
        "\n",
        "Epoch 1: val_loss improved from inf to 0.33582, saving model to best_model.h5\n"
    ]
    },
    {
    "name": "stderr",
    "output_type": "stream",
    "text": [
        "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
    ]
    },
    {
    "name": "stdout",
    "output_type": "stream",
    "text": [
        "49929/49929 - 70s - 1ms/step - loss: 0.4076 - mse_real: 2322.6353 - rmse_real: 44.5129 - val_loss: 0.3358 - val_mse_real: 1644.1116 - val_rmse_real: 40.4637\n",
        "Epoch 2/100\n",
        "\n",
        "Epoch 2: val_loss improved from 0.33582 to 0.33579, saving model to best_model.h5\n"
    ]
    },
    {
    "name": "stderr",
    "output_type": "stream",
    "text": [
        "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
    ]
    },
    {
    "name": "stdout",
    "output_type": "stream",
    "text": [
        "49929/49929 - 67s - 1ms/step - loss: 0.3356 - mse_real: 1637.5887 - rmse_real: 40.3873 - val_loss: 0.3358 - val_mse_real: 1641.7657 - val_rmse_real: 40.4351\n",
        "Epoch 3/100\n",
        "\n",
        "Epoch 3: val_loss improved from 0.33579 to 0.33577, saving model to best_model.h5\n"
    ]
    },
    {
    "name": "stderr",
    "output_type": "stream",
    "text": [
        "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
    ]
    },
    {
    "name": "stdout",
    "output_type": "stream",
    "text": [
        "49929/49929 - 69s - 1ms/step - loss: 0.3355 - mse_real: 1637.4263 - rmse_real: 40.3849 - val_loss: 0.3358 - val_mse_real: 1641.9222 - val_rmse_real: 40.4370\n",
        "Epoch 4/100\n",
        "\n",
        "Epoch 4: val_loss did not improve from 0.33577\n",
        "49929/49929 - 70s - 1ms/step - loss: 0.3355 - mse_real: 1637.3541 - rmse_real: 40.3839 - val_loss: 0.3358 - val_mse_real: 1645.2397 - val_rmse_real: 40.4775\n",
        "Epoch 5/100\n",
        "\n",
        "Epoch 5: val_loss did not improve from 0.33577\n",
        "49929/49929 - 69s - 1ms/step - loss: 0.3355 - mse_real: 1637.3877 - rmse_real: 40.3846 - val_loss: 0.3358 - val_mse_real: 1643.6863 - val_rmse_real: 40.4585\n",
        "Epoch 6/100\n",
        "\n",
        "Epoch 6: val_loss did not improve from 0.33577\n",
        "49929/49929 - 70s - 1ms/step - loss: 0.3355 - mse_real: 1637.3088 - rmse_real: 40.3837 - val_loss: 0.3358 - val_mse_real: 1643.7808 - val_rmse_real: 40.4598\n",
        "Epoch 7/100\n",
        "\n",
        "Epoch 7: val_loss did not improve from 0.33577\n",
        "49929/49929 - 68s - 1ms/step - loss: 0.3355 - mse_real: 1637.2535 - rmse_real: 40.3830 - val_loss: 0.3358 - val_mse_real: 1644.6272 - val_rmse_real: 40.4701\n",
        "Epoch 8/100\n",
        "\n",
        "Epoch 8: val_loss did not improve from 0.33577\n",
        "49929/49929 - 66s - 1ms/step - loss: 0.3355 - mse_real: 1637.3075 - rmse_real: 40.3835 - val_loss: 0.3358 - val_mse_real: 1643.8696 - val_rmse_real: 40.4608\n",
        "Epoch 8: early stopping\n",
        "Restoring model weights from the end of the best epoch: 3.\n",
        "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 538us/step\n"
    ]
    }
    ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1. Data Loading & Split\n",
    "# Training files\n",
    "train_files = ['playground-series-s5e2/train.csv', 'playground-series-s5e2/training_extra.csv']\n",
    "train_dfs = [pd.read_csv(f) for f in train_files]\n",
    "# Test file\n",
    "df_test = pd.read_csv('playground-series-s5e2/test.csv')\n",
    "# Infer id and target columns from sample submission\n",
    "sub_sample = pd.read_csv('playground-series-s5e2/sample_submission.csv', nrows=0)\n",
    "id_col = sub_sample.columns[0]\n",
    "target_columns = sub_sample.columns.tolist()[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Encoding (Regression)\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 3. Features & IDs\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Drop columns with all missing values\n",
    "X_train.dropna(axis=1, how='all', inplace=True)\n",
    "X_val = X_val[X_train.columns]\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cats = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, low_card_cats)\n",
    "])\n",
    "# Fit & transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# 6. Model Architecture Selection\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = len(target_columns)\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units1 = min(n_features * 2, 128)\n",
    "    units2 = min(n_features, 64)\n",
    "    hidden_layers = [int(units1), int(units2)]\n",
    "    use_bn = False\n",
    "    dropout_rate = 0.3\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_layers = [int(s) for s in sizes if s >= 16]\n",
    "    use_bn = True\n",
    "    dropout_rate = 0.4\n",
    "\n",
    "# Build the model\n",
    "inputs = Input(shape=(n_features,))\n",
    "x = inputs\n",
    "for units in hidden_layers:\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    if use_bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(n_targets, activation='linear')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "def mse_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "mse_real.__name__ = 'mse_real'      \n",
    "\n",
    "def rmse_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "rmse_real.__name__ = 'rmse_real'\n",
    "\n",
    "# 7. Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[rmse_real, mse_real]\n",
    ")\n",
    "\n",
    "# 8. Callbacks & Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_proc, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# 9. Evaluation & Logging\n",
    "training_loss = history.history['mse_real'][-1]\n",
    "training_rmse = history.history['rmse_real'][-1]\n",
    "validation_loss = history.history['val_mse_real'][-1]\n",
    "validation_rmse = history.history['val_rmse_real'][-1]\n",
    "results = {\n",
    "    'training_loss': training_loss,\n",
    "    'training_rmse': training_rmse,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_rmse': validation_rmse,\n",
    "    'training_duration': duration\n",
    "}\n",
    "with open('Keras/results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 10. Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(final)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('Keras/submission_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner - 1 Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [01h 17m 39s]\n",
      "val_loss: 0.33597391843795776\n",
      "\n",
      "Best val_loss So Far: 0.335781455039978\n",
      "Total elapsed time: 14h 03m 27s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99858/99858 - 194s - 2ms/step - loss: 0.3556 - mse_real: 2132.3987 - rmse_real: 41.5539 - val_loss: 0.3360 - val_mse_real: 1641.9176 - val_rmse_real: 40.3551\n",
      "Epoch 2/100\n",
      "99858/99858 - 185s - 2ms/step - loss: 0.3357 - mse_real: 1638.3668 - rmse_real: 40.3152 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 3/100\n",
      "99858/99858 - 178s - 2ms/step - loss: 0.3357 - mse_real: 1638.3527 - rmse_real: 40.3149 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 4/100\n",
      "99858/99858 - 194s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 5/100\n",
      "99858/99858 - 184s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 6/100\n",
      "99858/99858 - 178s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 7/100\n",
      "99858/99858 - 181s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 8/100\n",
      "99858/99858 - 178s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 9/100\n",
      "99858/99858 - 185s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 10/100\n",
      "99858/99858 - 177s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "Epoch 11/100\n",
      "99858/99858 - 181s - 2ms/step - loss: 0.3357 - mse_real: 1638.3419 - rmse_real: 40.3148 - val_loss: 0.3360 - val_mse_real: 1642.1677 - val_rmse_real: 40.3581\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 631us/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1. Data Loading & Split\n",
    "# Training files\n",
    "train_files = ['playground-series-s5e2/train.csv', 'playground-series-s5e2/training_extra.csv']\n",
    "train_dfs = [pd.read_csv(f) for f in train_files]\n",
    "# Test file\n",
    "df_test = pd.read_csv('playground-series-s5e2/test.csv')\n",
    "# Infer id and target columns from sample submission\n",
    "sub_sample = pd.read_csv('playground-series-s5e2/sample_submission.csv', nrows=0)\n",
    "id_col = sub_sample.columns[0]\n",
    "target_columns = sub_sample.columns.tolist()[1:]\n",
    "\n",
    "# Combine training data\n",
    "df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Encoding (Regression)\n",
    "y_values = df[target_columns].astype(float).values\n",
    "y_enc = np.log1p(y_values) if np.all(y_values >= 0) else y_values\n",
    "\n",
    "# 3. Features & IDs\n",
    "X = df.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "X_train = X.copy()\n",
    "y_train = y_enc\n",
    "test_ids = df_test[id_col]\n",
    "X_val = df_test.drop(columns=target_columns + [id_col], errors='ignore')\n",
    "y_val = None\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Drop columns with all missing values\n",
    "X_train.dropna(axis=1, how='all', inplace=True)\n",
    "X_val = X_val[X_train.columns]\n",
    "# Identify categorical vs numeric\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "low_card_cats = [c for c in categorical_cols if X_train[c].nunique() <= 50]\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# 5. Preprocessing Pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, low_card_cats)\n",
    "])\n",
    "# Fit & transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc = preprocessor.transform(X_val)\n",
    "\n",
    "# 6. Model Architecture Selection\n",
    "n_samples, n_features = X_train_proc.shape\n",
    "n_targets = len(target_columns)\n",
    "if n_samples < 10000 or n_features < 100:\n",
    "    units1 = min(n_features * 2, 128)\n",
    "    units2 = min(n_features, 64)\n",
    "    hidden_layers = [int(units1), int(units2)]\n",
    "    use_bn = False\n",
    "    dropout_rate = 0.3\n",
    "else:\n",
    "    sizes = [n_features * i for i in (2, 1, 0.5, 0.25)]\n",
    "    hidden_layers = [int(s) for s in sizes if s >= 16]\n",
    "    use_bn = True\n",
    "    dropout_rate = 0.4\n",
    "\n",
    "# Build the model using Keras Tuner\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "n_features = X_train_proc.shape[1]\n",
    "\n",
    "def mse_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "mse_real.__name__ = 'mse_real'      \n",
    "\n",
    "def rmse_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "rmse_real.__name__ = 'rmse_real'\n",
    "\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        layers = hp.Int('layers', 2, 8)\n",
    "        units = hp.Int('units', 64, 1024, 64)\n",
    "        drop = hp.Float('dropout', 0.0, 0.5, 0.1)\n",
    "        opt = hp.Choice('optimizer', ['adam'])\n",
    "        lr = hp.Float('learning_rate', 1e-5, 0.01, sampling='log')\n",
    "\n",
    "        inputs = Input(shape=(n_features,))\n",
    "        x = inputs\n",
    "        for _ in range(layers):\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=opt, loss='mean_squared_error', metrics=[rmse_real,mse_real])\n",
    "        return model\n",
    "\n",
    "# Initialize the Bayesian tuner\n",
    "bs = 32  # batch size\n",
    "ep = 20   # epochs\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    MyHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=42,\n",
    "    overwrite=False,\n",
    "    project_name='bayesian_tuner'\n",
    ")\n",
    "\n",
    "if y_val is not None:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "else:\n",
    "    tuner.search(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=bs, epochs=ep,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )\n",
    "\n",
    "model = tuner.hypermodel.build(tuner.get_best_hyperparameters(1)[0])\n",
    "\n",
    "# Retrain the model with the original callbacks and data\n",
    "start_time = time.time()  # Start timing\n",
    "if y_val is not None:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_data=(X_val_proc, y_val),\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        X_train_proc, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100, batch_size=bs,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        verbose=2\n",
    "    )\n",
    "end_time = time.time()  # End timing\n",
    "duration = end_time - start_time  # Calculate duration\n",
    "\n",
    "# 9. Evaluation & Logging\n",
    "training_loss = history.history['mse_real'][-1]\n",
    "training_rmse = history.history['rmse_real'][-1]\n",
    "validation_loss = history.history['val_mse_real'][-1]\n",
    "validation_rmse = history.history['val_rmse_real'][-1]\n",
    "results = {\n",
    "    'training_loss': training_loss,\n",
    "    'training_rmse': training_rmse,\n",
    "    'validation_loss': validation_loss,\n",
    "    'validation_rmse': validation_rmse,\n",
    "    'training_duration': duration\n",
    "}\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# 10. Prediction & Submission\n",
    "raw_preds = model.predict(X_val_proc)\n",
    "final = raw_preds\n",
    "if np.all(final >= 0):\n",
    "    final = np.expm1(final)\n",
    "if final.ndim == 1:\n",
    "    final = final.reshape(-1, 1)\n",
    "submission = pd.DataFrame(final, columns=target_columns)\n",
    "submission.insert(0, id_col, test_ids.reset_index(drop=True))\n",
    "submission.to_csv('submission_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
